{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GeneradorCSS.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zelechos/Generador-Css/blob/main/GeneradorCSS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bN15gvKIX-Ha"
      },
      "source": [
        "# **Proyecto Generador de CSS [v0.0.8]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VnNOPgW4JHk"
      },
      "source": [
        "# Clonamos nuestro repositorio en donde se encuentra nuestro dataset con este comando"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbRrBcSn4IRF",
        "outputId": "7430a00d-22f5-4614-97be-3356d28952a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://github.com/Zelechos/Generador-Css.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Generador-Css'...\n",
            "remote: Enumerating objects: 58, done.\u001b[K\n",
            "remote: Counting objects: 100% (58/58), done.\u001b[K\n",
            "remote: Compressing objects: 100% (54/54), done.\u001b[K\n",
            "remote: Total 58 (delta 21), reused 13 (delta 3), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (58/58), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3rBPXXFuelK"
      },
      "source": [
        "### en este caso no es necesario ya que tenemos el dataset en github por ende tenemos que trabajar con el dataset que esta en formato json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD_besEJ5uxB"
      },
      "source": [
        "### Accedemos a los datos del dataset TEST.json\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUUjEn8C5q1p",
        "outputId": "f6466512-2e8e-427e-daa0-c0c292b4e251",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import json\n",
        "\n",
        "with open('/content/Generador-Css/dataset/TEST.json') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "#accedemos a la los datos\n",
        "data_styles = data[\"styles\"]\n",
        "print(data_styles[0])\n",
        "\n",
        "\n",
        "#accediendo a los estilos\n",
        "styles = data_styles[0][\"style\"]\n",
        "print(\"\\nstyles access => \",styles)\n",
        "\n",
        "\n",
        "#accediendo a los selectores\n",
        "selector = data_styles[0][\"selector\"]\n",
        "print(\"\\nselector access => \",selector)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'style': 'body{overflow:hidden;background:rgb(25,35,125)}', 'selector': 'body'}\n",
            "\n",
            "styles access =>  body{overflow:hidden;background:rgb(25,35,125)}\n",
            "\n",
            "selector access =>  body\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyLQIJ6A6Qom"
      },
      "source": [
        "### Creamos un array para poder mandar los datos y luego tranformarlos en numeros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzfDVXgs6Xu2",
        "outputId": "226fe663-f789-4cb0-d189-8d876c1f526b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data_css = []\n",
        "styles = []\n",
        "selectors = []\n",
        "for data in data_styles:\n",
        "  selectors.append(data[\"selector\"])\n",
        "  styles.append(data[\"style\"])\n",
        " \n",
        "# print(selectors)\n",
        "# print(styles)\n",
        "data_css.append(selectors)\n",
        "data_css.append(styles)\n",
        "print(data_css[0])\n",
        "print(data_css[1])\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['body', 'div.drop-container', 'div.drop', 'h1', '@font-face', 'body', '.logo', '.switch-left', '.pad-left', '.switch-right', '.pad-right', '.line', '.text', '.text', '.text', '.reload', '.logo', 'html,body', 'body', '*', '*', '*', '.css_lab', '.witch', 'body', 'body img', 'img', 'body h1', 'h1', 'body a', 'a', '@keyframes glow', 'from', 'to', 'body', 'h1', 'ul', 'li', 'li:hover span', 'li.invisible', 'li.animate', 'span', 'span:before', 'html', 'body', 'header', 'article', 'h3,section', 'h3', 'section', 'h3:before', 'h3:before,section:before', 'section:before', 'h3', 'section', 'h3:before', 'section:before', 'p', 'a', 'body,html', 'body', 'html', 'div.codepen', 'div', '.codepen', '*', 'body', 'aside.context', '.context', 'aside', 'a', 'a:hover', 'footer', 'footer a', 'a']\n",
            "['body{overflow:hidden;background:rgb(25,35,125)}', 'div.drop-container{position:absolute;top:0;right:0;bottom:0;left:0;margin:auto;height:200px;width:200px}', 'div.drop{position:absolute;top:-25%;width:100%;height:100%;border-radius:100% 5% 100% 100%;-webkit-transform:rotate(-45deg);transform:rotate(-45deg);margin:0;background:deepskyblue;-webkit-animation:drip 4s forwards;animation:drip 4s forwards}', 'h1{color:#fff;position:absolute;font-size:2.5em;height:1em;top:0;left:0;right:0;bottom:0;z-index:2;margin:auto;text-align:center;opacity:0;-webkit-animation:appear 2s 2.5s forwards;animation:appear 2s 2.5s forwards}', '@font-face {font-family: logofont;src: url(https://s3-us-west-2.amazonaws.com/s.cdpn.io/531144/couture-bld.otf);}', 'body {margin: 0px;background: #EF5350;}', '.logo {transform: scale(0.7);margin-top: 45px;}', '.switch-left{position:fixed;background:transparent;width:130px;height:380px;border-radius:100px 5px 5px 100px;border:35px solid #fff;transform:translate(calc(50vw - 217px),calc(50vh - 225px));animation:switch-left-animation 1.75s ease}', '.pad-left{position:fixed;width:80px;height:80px;background:#fff;border-radius:100%;margin-top:45px;margin-left:27.5px}', '.switch-right{position:fixed;background:#fff;width:185px;height:450px;border-radius:5px 100px 100px 5px;transform:translate(calc(50vw + 17px),calc(50vh - 225px));animation:switch-right-animation 1.75s ease}', '.pad-right{position:fixed;width:80px;height:80px;background:#EF5350;border-radius:100%;margin-top:225px;margin-left:50px}', '.line{width:6px;height:100vh;position:fixed;background:#fff;top:0;left:calc(50vw - 3px);opacity:.25}', '.text{font-family:logofont;font-size:80px;color:#fff;letter-spacing:19px;position:fixed;transform:translate(calc(50vw - 288px),calc(50vh + 175px))}', '.text{font-family:logofont;font-size:124px;color:#fff;letter-spacing:13px;position:fixed;transform:translate(calc(50vw - 290px),calc(50vh + 200px))}', '.text{animation: text-animation 1.75s ease;position: fixed;}', '.reload{position:fixed;width:75px;height:75px;border-radius:5px;left:calc(50vw - 37.5px);bottom:calc(50vh - 345px);cursor:pointer;transition:background 0.3s ease;animation:reload-animation 2.5s ease}', ' .logo{transform: scale(0.45);margin-top: 65px;}', 'html,body{margin:0;padding:0;width:100%;height:100%;overflow:hidden;}', 'body{margin:0;padding:0;width:100%;height:100%;overflow:hidden;}', '*{margin:0;padding:0}', '*{margin:0;padding:0;box-sizing:border-box;}', '*{box-sizing:border-box;}', '.css_lab{width:100%;height:100%;background:#224}', '.witch{width:100px;height:100px;position:absolute;top:50%;left:50%;margin:-60px;-webkit-transform:scale(3);-ms-transform:scale(3);transform:scale(3)}', 'body{background-color:#212121;color:#4db1bc;margin:0;display:grid;grid-template-columns:1fr;grid-template-rows:1fr;align-items:center;justify-items:center}', 'body img{width:100%;height:100vh;object-fit:cover;grid-column:1;grid-row:1;opacity:.7;z-index:0}', 'img{width:100%;height:100vh;object-fit:cover;grid-column:1;grid-row:1;opacity:.7;z-index:0}', \"body h1{margin:0;padding-bottom:6rem;grid-column:1;grid-row:1;z-index:1;font-family:'Teko',sans-serif;font-size:10rem;text-transform:uppercase;animation:glow 2s ease-in-out infinite alternate;text-align:center}\", \"h1{margin:0;padding-bottom:6rem;grid-column:1;grid-row:1;z-index:1;font-family:'Teko',sans-serif;font-size:10rem;text-transform:uppercase;animation:glow 2s ease-in-out infinite alternate;text-align:center}\", \"body a{font-family:'Teko',sans-serif;color:#4db1bc;grid-column:1;grid-row:1;align-self:end;justify-self:center;padding-bottom:1rem}\", \"a{font-family:'Teko',sans-serif;color:#4db1bc;grid-column:1;grid-row:1;align-self:end;justify-self:center;padding-bottom:1rem}\", '@keyframes glow{from{text-shadow:0 0 20px #2d9da9}to{text-shadow:0 0 30px #34b3c1,0 0 10px #4dbbc7}}', 'from{text-shadow:0 0 20px #2d9da9}', 'to{text-shadow:0 0 30px #34b3c1,0 0 10px #4dbbc7}', \"body{font-size:16px;color:#fff;background:#C13237;font-family:'Raleway',arial,sans-serif}\", 'h1{text-align:center;font-size:6em;font-weight:300;position:absolute;margin:0;top:50%;left:50%;-webkit-transform:translate(-50%,-50%);transform:translate(-50%,-50%)}', 'ul{padding-left:1em;position:fixed;bottom:0;font-weight:400;z-index:100;min-width:20em}', 'li{list-style:none;margin:.5em 0 0;font-size:1.75em}', 'li:hover span{opacity:1;transition:0.25s}', 'li.invisible{opacity:0;visibility:hidden;transition:all 0.25s}', 'li.animate{opacity:0;animation-duration:0.25s;animation-name:easeOutBounce;animation-fill-mode:forwards}', 'span{color:#C13237;padding:.35em;margin-left:.5em;border-radius:.2em;font-size:.75em;transition:.3s;background:#fff;opacity:0;position:relative}', \"span:before{content:'';width:0;height:0;position:absolute;left:-.4em;top:50%;margin-top:-.5em;border-style:solid;border-width:.5em .5em .5em 0;border-color:transparent #fff transparent transparent}\", 'html{--i:-1;--j:-1;--r:0;height:100%;background:url(https://images.unsplash.com/photo-1496481995273-1ba7de6c24fd?ixlib=rb-0.3.5&q=85&fm=jpg&crop=entropy&cs=srgb&ixid=eyJhcHBfaWQiOjE0NTg5fQ&s=63d82148afc0fe5a35f9752b4d511d82) 50%/cover #777;background-blend-mode:luminosity}', 'body{display:flex;flex-wrap:wrap;justify-content:center;margin:0 auto;padding-top:4em;max-width:1000px;font:1em/1.5 trebuchet ms,verdana,sans-serif}', 'header{margin:1rem;padding:1em 0;border-radius:.5em;min-width:95%;background:#fff;text-align:center}', 'article{overflow:hidden;margin:1rem;width:21em;min-width:15rem;border-radius:1em}', 'h3,section{display:flex;align-items:center;overflow:hidden;position:relative;padding:.5rem}', 'h3{display:flex;align-items:center;overflow:hidden;position:relative;padding:.5rem}', 'section{display:flex;align-items:center;overflow:hidden;position:relative;padding:.5rem}', \"h3:before{position:absolute;z-index:-1;top:calc(var(--j)*1rem + (1 + var(--j))*50% - var(--r));left:calc(var(--i)*1rem + (1 + var(--i))*50% - var(--r));padding:var(--r);border-radius:50%;box-shadow:0 0 0 50em;content:''}\", \"h3:before,section:before{position:absolute;z-index:-1;top:calc(var(--j)*1rem + (1 + var(--j))*50% - var(--r));left:calc(var(--i)*1rem + (1 + var(--i))*50% - var(--r));padding:var(--r);border-radius:50%;box-shadow:0 0 0 50em;content:''}\", \"section:before{position:absolute;z-index:-1;top:calc(var(--j)*1rem + (1 + var(--j))*50% - var(--r));left:calc(var(--i)*1rem + (1 + var(--i))*50% - var(--r));padding:var(--r);border-radius:50%;box-shadow:0 0 0 50em;content:''}\", 'h3{justify-content:center;color:#fff;font-size:1.75em;text-align:center;min-height:var(--r)}', 'section{justify-content:space-between;min-height:calc(var(--r) - 1rem)}', 'h3:before{opacity:.65}', 'section:before{color:var(--c0)}', 'p{margin-right:1em;font-size:.875em}', 'a{display:inline-block;color:inherit;text-decoration:none;text-transform:uppercase;white-space:nowrap}', 'body,html{background-image:linear-gradient(90deg,#1d1f20,#2f3031,#1d1f20);height:100%}', 'body{background-image:linear-gradient(90deg,#1d1f20,#2f3031,#1d1f20);height:100%}', 'html{background-image:linear-gradient(90deg,#1d1f20,#2f3031,#1d1f20);height:100%}', 'div.codepen{display:block;position:relative;top:50%;height:16em;width:16em;margin:-8em auto 0;border-radius:8em;background-color:#121212;cursor:pointer;transition:color 0.8s linear,background 0.8s linear,box-shadow 0.1s linear,transform 0.1s linear}', 'div{display:block;position:relative;top:50%;height:16em;width:16em;margin:-8em auto 0;border-radius:8em;background-color:#121212;cursor:pointer;transition:color 0.8s linear,background 0.8s linear,box-shadow 0.1s linear,transform 0.1s linear}', '.codepen{display:block;position:relative;top:50%;height:16em;width:16em;margin:-8em auto 0;border-radius:8em;background-color:#121212;cursor:pointer;transition:color 0.8s linear,background 0.8s linear,box-shadow 0.1s linear,transform 0.1s linear}', '*{box-sizing:border-box}', \"body{background:#222;font-family:'Gochi Hand',sans-serif;color:#333}\", 'aside.context{text-align:center;color:#fff;line-height:1.7;font-size:20px;letter-spacing:.5px}', '.context{text-align:center;color:#fff;line-height:1.7;font-size:20px;letter-spacing:.5px}', 'aside{text-align:center;color:#fff;line-height:1.7;font-size:20px;letter-spacing:.5px}', 'a{text-decoration:none;color:#fff;padding:3px 0;border-bottom:1px dashed}', 'a:hover{border-bottom:1px solid}', 'footer{text-align:center;margin:4em auto;width:100%}', 'footer a{text-decoration:none;display:inline-block;width:45px;height:45px;border-radius:50%;background:transparent;border:1px dashed #fff;color:#fff;margin:5px}', 'a{text-decoration:none;display:inline-block;width:45px;height:45px;border-radius:50%;background:transparent;border:1px dashed #fff;color:#fff;margin:5px}']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uZJoUBfwV0U"
      },
      "source": [
        "### Convertimos los datos a numeros para trabajar\n",
        "\n",
        "### Pasos a Seguir para construir mi Custom Dataset\n",
        "1.   Contruir el vocabulario para asignar a cada palabra\n",
        "2.   Configurar un dataset de Pytorch para cargar nuestro datos\n",
        "2.   Configurar el batch para que todos los ejemplos tengan las misma longitud (seq_len) y tambien configuramos el dataloader\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2zqHb7Uxw-S"
      },
      "source": [
        "## Importaciones requeridas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbu8Hq1ywabl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "511324d6-c12f-4d89-f941-ee4b006f3035"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import spacy # para la tokenizacion\n",
        "import pandas as pd # para buscar en el archivo de anotaciones (atento)\n",
        "from torch.nn.utils.rnn import pad_sequence #para el lote de almohadillas (pad batch) para que todos los datos tengan la misma longitud\n",
        "from torch.utils.data import DataLoader , Dataset #para crea el dataset y el dataloader para el entrenamiento\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clase para crear nuestro vocabulario"
      ],
      "metadata": {
        "id": "YyqCRf_NzLHX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAyccPMCK3iu"
      },
      "source": [
        "spacy_eng = spacy.load(\"en\")\n",
        "class Vocabulary:\n",
        "\n",
        "  def __init__(self, freq_threshold):\n",
        "    self.itos = {0:\"<PAD>\", 1:\"<SOS>\", 2:\"<EOS>\", 3:\"<UNK>\"}\n",
        "    self.stoi = {\"<PAD>\":0, \"<SOS>\":1, \"<EOS>\":2, \"<UNK>\":3}\n",
        "    self.freq_threshold = freq_threshold\n",
        "\n",
        "  def __len__(self):\n",
        "    #devolvemos la longitud de nuestro vocabulario\n",
        "    return len(self.itos)\n",
        "\n",
        "  #metodo para definir el ingles de nuestro vocabulario\n",
        "  @staticmethod\n",
        "  def tokenizer_eng(text):\n",
        "    return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
        "\n",
        "  #ESTO ES LO QUE HACER EJEMPLO!!!\n",
        "  # \"body{background-color:red;}\"\" => [\"body\",\"{\",\"background\",\"-\",\"color\",\":\",\"red\",\";\",\"}\"]\n",
        "\n",
        "  #construimos el vocabulario!!\n",
        "  def build_vocabulary(self, sentence_list):\n",
        "    frequencies = {}\n",
        "    idx = 4\n",
        "\n",
        "    for sentence in sentence_list:\n",
        "      for word in self.tokenizer_eng(sentence):\n",
        "        if word not in frequencies:\n",
        "          frequencies[word] = 1\n",
        "        else:\n",
        "          frequencies[word] += 1\n",
        "\n",
        "        if frequencies[word] == self.freq_threshold:\n",
        "          self.stoi[word] = idx\n",
        "          self.itos[idx] = word\n",
        "          idx += 1\n",
        "\n",
        "  def numericalize(self, text):\n",
        "    tokenized_text = self.tokenizer_eng(text)\n",
        "    return [\n",
        "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
        "            for token in tokenized_text\n",
        "    ]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-7wWWh3K4Hz"
      },
      "source": [
        "### Clase CssDataset para clasificar el conjunto de datos nuestro dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYe-StziK8MM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "945347a6-c4f4-43cc-ac70-398c3c6cd055"
      },
      "source": [
        "class CssDataset(Dataset):\n",
        "\n",
        "  def __init__ (self, css_styles, freq_threshold = 5):\n",
        "    self.df = css_styles\n",
        "\n",
        "    # vamos a obtener nuestros selectores\n",
        "    self.selectors = self.df[0]\n",
        "\n",
        "    # vamos a obtener nuestros styles\n",
        "    self.styles = self.df[1]\n",
        "    \n",
        "    #Incializamos el Vocabulario (IMPORTANTEE!!!)\n",
        "    self.vocab = Vocabulary(freq_threshold)\n",
        "    self.vocab1 = Vocabulary(freq_threshold)\n",
        "\n",
        "    self.vocab.build_vocabulary(self.styles) #Pasamos las Listas a el vocabulario\n",
        "    self.vocab1.build_vocabulary(self.selectors) #Pasamos las Listas a el vocabulario\n",
        "    \n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df[1]) #devolvemos la longitud de nuestros dataset\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    selector = self.selectors[index]\n",
        "    style = self.styles[index]\n",
        "\n",
        "    numericalized_style = [self.vocab.stoi[\"<SOS>\"]] \n",
        "    numericalized_style += self.vocab.numericalize(style) \n",
        "    numericalized_style.append(self.vocab.stoi[\"<EOS>\"]) \n",
        "\n",
        "    numericalized_selector = [self.vocab1.stoi[\"<SOS>\"]] \n",
        "    numericalized_selector += self.vocab1.numericalize(selector) \n",
        "    numericalized_selector.append(self.vocab1.stoi[\"<EOS>\"]) \n",
        "\n",
        "    return torch.tensor(numericalized_selector) , torch.tensor(numericalized_style) \n",
        "\n",
        "\n",
        "train = CssDataset(data_css)\n",
        "len(train)\n",
        "print(type(train))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class '__main__.CssDataset'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clase para cargar los datos"
      ],
      "metadata": {
        "id": "SqLMlAamBvzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyCollate:\n",
        "\n",
        "  def __init__(self, pad_idx):\n",
        "    self.pad_idx = pad_idx\n",
        "\n",
        "  def __call__(self, batch):\n",
        "    targets = [item[0] for item in batch]\n",
        "    targets = pad_sequence(targets, batch_first = False, padding_value = self.pad_idx )\n",
        "\n",
        "    targets1 = [item[1] for item in batch]\n",
        "    targets1 = pad_sequence(targets1, batch_first = False, padding_value = self.pad_idx )\n",
        "\n",
        "    return targets, targets1\n",
        "\n",
        "def get_loader(css_styles, batch_size = 32, num_workers = 8, shuffle = True, pin_memory = True):\n",
        "  dataset = CssDataset(css_styles)\n",
        "  pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
        "\n",
        "  dataloader = DataLoader(\n",
        "      dataset = dataset,\n",
        "      batch_size = batch_size,\n",
        "      num_workers = num_workers,\n",
        "      shuffle= shuffle,\n",
        "      pin_memory=pin_memory,\n",
        "      collate_fn=MyCollate(pad_idx=pad_idx),\n",
        "  )\n",
        "\n",
        "  return dataset, dataloader\n",
        "\n",
        "def main():\n",
        "  dataset, dataloader = get_loader(data_css)\n",
        "  print(dataloader)\n",
        "  print(dataset)\n",
        "  # for idx , (selectors, styles) in enumerate(dataloader):\n",
        "    # print(selectors.shape)\n",
        "    # print(styles.shape)\n",
        "    # print(selectors)\n",
        "    # print(styles)\n",
        "    # print(type(selectors))\n",
        "    # print(type(styles))\n",
        "    # print(idx)\n",
        "\n",
        "if __name__ == \"__main__\" :\n",
        "  main()\n",
        "\n"
      ],
      "metadata": {
        "id": "HZhlq6IcBhhO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd920687-cfcf-4407-b546-757fa622a681"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<torch.utils.data.dataloader.DataLoader object at 0x7f94588081d0>\n",
            "<__main__.CssDataset object at 0x7f94587ec350>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Usaremos Perceptrón Multicapa (MLP) para el Generador y Discriminador"
      ],
      "metadata": {
        "id": "uY9wXuXeJgFT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CharRNN(torch.nn.Module):\n",
        "  def __init__(self, input_size, embedding_size=128, hidden_size=256, num_layers=2, dropout=0.2):\n",
        "    super().__init__()\n",
        "    self.encoder = torch.nn.Embedding(input_size, embedding_size)\n",
        "    self.rnn = torch.nn.LSTM(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout, batch_first=True)\n",
        "    self.fc = torch.nn.Linear(hidden_size, input_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.encoder(x)\n",
        "    x, h = self.rnn(x)         \n",
        "    y = self.fc(x[:,-1,:])\n",
        "    return y"
      ],
      "metadata": {
        "id": "2aSp8BfEJnVD"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CharRNN(input_size=114)\n",
        "outputs = model(torch.randint(0, 114, (64, 50)))\n",
        "outputs.shape"
      ],
      "metadata": {
        "id": "pa24rCygeEf1",
        "outputId": "28fcfe70-b69e-4229-808c-ac92aeb5d55b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 114])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def fit(model, dataloader, epochs=10):\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        train_loss = []\n",
        "        bar = tqdm(dataloader)\n",
        "        for batch in bar:\n",
        "            X, y = batch\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            y_hat = model(X)\n",
        "            loss = criterion(y_hat, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss.append(loss.item())\n",
        "            bar.set_description(f\"loss {np.mean(train_loss):.5f}\")\n",
        "        bar = tqdm(dataloader)\n",
        "        val_loss = []\n",
        "        with torch.no_grad():\n",
        "            for batch in bar:\n",
        "                X, y = batch\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                y_hat = model(X)\n",
        "                loss = criterion(y_hat, y)\n",
        "                val_loss.append(loss.item())\n",
        "                bar.set_description(f\"val_loss {np.mean(val_loss):.5f}\")\n",
        "        print(f\"Epoch {epoch}/{epochs} loss {np.mean(train_loss):.5f} val_loss {np.mean(val_loss):.5f}\")\n",
        "\n",
        "def predict(model, X):\n",
        "    model.eval() \n",
        "    with torch.no_grad():\n",
        "        X = torch.tensor(X).to(device)\n",
        "        pred = model(X.unsqueeze(0))\n",
        "        return pred"
      ],
      "metadata": {
        "id": "K3i7o5Bvfu-K"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset, dataloader = get_loader(data_css)\n",
        "print(len(dataloader))\n",
        "print(type(dataloader))\n",
        "i , ss , sss = dataloader\n",
        "\n",
        "# print(sss)\n",
        "model = CharRNN(input_size=114)\n",
        "fit(model, dataloader, epochs=20)"
      ],
      "metadata": {
        "id": "nhPSQ0dof2rx",
        "outputId": "f2f08281-a807-4472-82b5-eaf346874c54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "<class 'torch.utils.data.dataloader.DataLoader'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-2cbfeca390f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# print(sss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCharRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m114\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-54-1a6a73af1adc>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, dataloader, epochs)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m   1151\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                                label_smoothing=self.label_smoothing)\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   2844\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2845\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2846\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (6) to match target batch_size (36)."
          ]
        }
      ]
    }
  ]
}