{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zelechos/Generador-Css/blob/main/PruebaRNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS35G8o3ryWv"
      },
      "source": [
        "En este post vamos a entrenar una `red neuronal recurrente` para generar texto, carácter a carácter, inspirado en [CharRNN](https://github.com/karpathy/char-rnn). Nuestra red neuronal recibirá como entrada una secuencia de letras y deberá dar como salida la siguiente letra (la cual añadiremos a las entradas para volver a generar un nuevo carácter). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FezAAhnRryWv"
      },
      "source": [
        "## Los datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJ97bGHgryWw"
      },
      "source": [
        "Lo primero que necesitamos para lograr nuestro objetivo es un conjunto de datos. En este caso, al querer generar texto, nos servirá con un archivo con mucho texto que queramos imitar. Para ello descargaremos *Don Quijote de la Mancha*, la obra principal del escritor Miguel de Cervantes y una de las más relevantes en la literatura castellana. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-31T14:37:39.405968Z",
          "start_time": "2020-08-31T14:37:38.446954Z"
        },
        "id": "iCDnTx3GryWw",
        "outputId": "3e7c5f3f-d13f-4bcc-f061-1410c0d04fcf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Generador-Css'...\n",
            "remote: Enumerating objects: 61, done.\u001b[K\n",
            "remote: Counting objects: 100% (61/61), done.\u001b[K\n",
            "remote: Compressing objects: 100% (57/57), done.\u001b[K\n",
            "remote: Total 61 (delta 23), reused 13 (delta 3), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (61/61), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Zelechos/Generador-Css.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-31T14:37:39.437968Z",
          "start_time": "2020-08-31T14:37:39.406971Z"
        },
        "id": "Bvle0plDryWz",
        "outputId": "5009cb3e-de23-4e2a-a114-a51e06f24a07"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('DON QUIJOTE DE LA MANCHA\\nMiguel de Cervantes Saavedra\\n\\nPRIMERA PARTE\\nCAPÍTULO 1: Que trata de la condición y ejercicio del famoso hidalgo D. Quijote de la Mancha\\nEn un lugar de la Mancha, de cuyo nombre no quiero acordarme, no ha mucho tiempo que vivía un hidalgo de los de lanza en astillero, ada',\n",
              " 1038397)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "f = open(\"el_quijote.txt\", \"r\", encoding='utf-8')\n",
        "text = f.read()\n",
        "text[:300], len(text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('/content/Generador-Css/dataset/TEST.json') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "#accedemos a la los datos\n",
        "data_styles = data[\"styles\"]\n",
        "print(data_styles[0])\n",
        "\n",
        "\n",
        "#accediendo a los estilos\n",
        "styles = data_styles[0][\"style\"]\n",
        "print(\"\\nstyles access => \",styles)\n",
        "\n",
        "\n",
        "#accediendo a los selectores\n",
        "selector = data_styles[0][\"selector\"]\n",
        "print(\"\\nselector access => \",selector)\n",
        "\n",
        "text = \"\"\n",
        "for styles in data_styles:\n",
        "  # print(styles['style'])\n",
        "  text += styles['style']\n",
        "\n",
        "print(text)"
      ],
      "metadata": {
        "id": "FcCvLZsNr90e",
        "outputId": "a9317a86-c6b0-4676-a866-2751b95f0f4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'style': 'body{overflow:hidden;background:rgb(25,35,125)}', 'selector': 'body'}\n",
            "\n",
            "styles access =>  body{overflow:hidden;background:rgb(25,35,125)}\n",
            "\n",
            "selector access =>  body\n",
            "body{overflow:hidden;background:rgb(25,35,125)}div.drop-container{position:absolute;top:0;right:0;bottom:0;left:0;margin:auto;height:200px;width:200px}div.drop{position:absolute;top:-25%;width:100%;height:100%;border-radius:100% 5% 100% 100%;-webkit-transform:rotate(-45deg);transform:rotate(-45deg);margin:0;background:deepskyblue;-webkit-animation:drip 4s forwards;animation:drip 4s forwards}h1{color:#fff;position:absolute;font-size:2.5em;height:1em;top:0;left:0;right:0;bottom:0;z-index:2;margin:auto;text-align:center;opacity:0;-webkit-animation:appear 2s 2.5s forwards;animation:appear 2s 2.5s forwards}@font-face {font-family: logofont;src: url(https://s3-us-west-2.amazonaws.com/s.cdpn.io/531144/couture-bld.otf);}body {margin: 0px;background: #EF5350;}.logo {transform: scale(0.7);margin-top: 45px;}.switch-left{position:fixed;background:transparent;width:130px;height:380px;border-radius:100px 5px 5px 100px;border:35px solid #fff;transform:translate(calc(50vw - 217px),calc(50vh - 225px));animation:switch-left-animation 1.75s ease}.pad-left{position:fixed;width:80px;height:80px;background:#fff;border-radius:100%;margin-top:45px;margin-left:27.5px}.switch-right{position:fixed;background:#fff;width:185px;height:450px;border-radius:5px 100px 100px 5px;transform:translate(calc(50vw + 17px),calc(50vh - 225px));animation:switch-right-animation 1.75s ease}.pad-right{position:fixed;width:80px;height:80px;background:#EF5350;border-radius:100%;margin-top:225px;margin-left:50px}.line{width:6px;height:100vh;position:fixed;background:#fff;top:0;left:calc(50vw - 3px);opacity:.25}.text{font-family:logofont;font-size:80px;color:#fff;letter-spacing:19px;position:fixed;transform:translate(calc(50vw - 288px),calc(50vh + 175px))}.text{font-family:logofont;font-size:124px;color:#fff;letter-spacing:13px;position:fixed;transform:translate(calc(50vw - 290px),calc(50vh + 200px))}.text{animation: text-animation 1.75s ease;position: fixed;}.reload{position:fixed;width:75px;height:75px;border-radius:5px;left:calc(50vw - 37.5px);bottom:calc(50vh - 345px);cursor:pointer;transition:background 0.3s ease;animation:reload-animation 2.5s ease} .logo{transform: scale(0.45);margin-top: 65px;}html,body{margin:0;padding:0;width:100%;height:100%;overflow:hidden;}body{margin:0;padding:0;width:100%;height:100%;overflow:hidden;}*{margin:0;padding:0}*{margin:0;padding:0;box-sizing:border-box;}*{box-sizing:border-box;}.css_lab{width:100%;height:100%;background:#224}.witch{width:100px;height:100px;position:absolute;top:50%;left:50%;margin:-60px;-webkit-transform:scale(3);-ms-transform:scale(3);transform:scale(3)}body{background-color:#212121;color:#4db1bc;margin:0;display:grid;grid-template-columns:1fr;grid-template-rows:1fr;align-items:center;justify-items:center}body img{width:100%;height:100vh;object-fit:cover;grid-column:1;grid-row:1;opacity:.7;z-index:0}img{width:100%;height:100vh;object-fit:cover;grid-column:1;grid-row:1;opacity:.7;z-index:0}body h1{margin:0;padding-bottom:6rem;grid-column:1;grid-row:1;z-index:1;font-family:'Teko',sans-serif;font-size:10rem;text-transform:uppercase;animation:glow 2s ease-in-out infinite alternate;text-align:center}h1{margin:0;padding-bottom:6rem;grid-column:1;grid-row:1;z-index:1;font-family:'Teko',sans-serif;font-size:10rem;text-transform:uppercase;animation:glow 2s ease-in-out infinite alternate;text-align:center}body a{font-family:'Teko',sans-serif;color:#4db1bc;grid-column:1;grid-row:1;align-self:end;justify-self:center;padding-bottom:1rem}a{font-family:'Teko',sans-serif;color:#4db1bc;grid-column:1;grid-row:1;align-self:end;justify-self:center;padding-bottom:1rem}@keyframes glow{from{text-shadow:0 0 20px #2d9da9}to{text-shadow:0 0 30px #34b3c1,0 0 10px #4dbbc7}}from{text-shadow:0 0 20px #2d9da9}to{text-shadow:0 0 30px #34b3c1,0 0 10px #4dbbc7}body{font-size:16px;color:#fff;background:#C13237;font-family:'Raleway',arial,sans-serif}h1{text-align:center;font-size:6em;font-weight:300;position:absolute;margin:0;top:50%;left:50%;-webkit-transform:translate(-50%,-50%);transform:translate(-50%,-50%)}ul{padding-left:1em;position:fixed;bottom:0;font-weight:400;z-index:100;min-width:20em}li{list-style:none;margin:.5em 0 0;font-size:1.75em}li:hover span{opacity:1;transition:0.25s}li.invisible{opacity:0;visibility:hidden;transition:all 0.25s}li.animate{opacity:0;animation-duration:0.25s;animation-name:easeOutBounce;animation-fill-mode:forwards}span{color:#C13237;padding:.35em;margin-left:.5em;border-radius:.2em;font-size:.75em;transition:.3s;background:#fff;opacity:0;position:relative}span:before{content:'';width:0;height:0;position:absolute;left:-.4em;top:50%;margin-top:-.5em;border-style:solid;border-width:.5em .5em .5em 0;border-color:transparent #fff transparent transparent}html{--i:-1;--j:-1;--r:0;height:100%;background:url(https://images.unsplash.com/photo-1496481995273-1ba7de6c24fd?ixlib=rb-0.3.5&q=85&fm=jpg&crop=entropy&cs=srgb&ixid=eyJhcHBfaWQiOjE0NTg5fQ&s=63d82148afc0fe5a35f9752b4d511d82) 50%/cover #777;background-blend-mode:luminosity}body{display:flex;flex-wrap:wrap;justify-content:center;margin:0 auto;padding-top:4em;max-width:1000px;font:1em/1.5 trebuchet ms,verdana,sans-serif}header{margin:1rem;padding:1em 0;border-radius:.5em;min-width:95%;background:#fff;text-align:center}article{overflow:hidden;margin:1rem;width:21em;min-width:15rem;border-radius:1em}h3,section{display:flex;align-items:center;overflow:hidden;position:relative;padding:.5rem}h3{display:flex;align-items:center;overflow:hidden;position:relative;padding:.5rem}section{display:flex;align-items:center;overflow:hidden;position:relative;padding:.5rem}h3:before{position:absolute;z-index:-1;top:calc(var(--j)*1rem + (1 + var(--j))*50% - var(--r));left:calc(var(--i)*1rem + (1 + var(--i))*50% - var(--r));padding:var(--r);border-radius:50%;box-shadow:0 0 0 50em;content:''}h3:before,section:before{position:absolute;z-index:-1;top:calc(var(--j)*1rem + (1 + var(--j))*50% - var(--r));left:calc(var(--i)*1rem + (1 + var(--i))*50% - var(--r));padding:var(--r);border-radius:50%;box-shadow:0 0 0 50em;content:''}section:before{position:absolute;z-index:-1;top:calc(var(--j)*1rem + (1 + var(--j))*50% - var(--r));left:calc(var(--i)*1rem + (1 + var(--i))*50% - var(--r));padding:var(--r);border-radius:50%;box-shadow:0 0 0 50em;content:''}h3{justify-content:center;color:#fff;font-size:1.75em;text-align:center;min-height:var(--r)}section{justify-content:space-between;min-height:calc(var(--r) - 1rem)}h3:before{opacity:.65}section:before{color:var(--c0)}p{margin-right:1em;font-size:.875em}a{display:inline-block;color:inherit;text-decoration:none;text-transform:uppercase;white-space:nowrap}body,html{background-image:linear-gradient(90deg,#1d1f20,#2f3031,#1d1f20);height:100%}body{background-image:linear-gradient(90deg,#1d1f20,#2f3031,#1d1f20);height:100%}html{background-image:linear-gradient(90deg,#1d1f20,#2f3031,#1d1f20);height:100%}div.codepen{display:block;position:relative;top:50%;height:16em;width:16em;margin:-8em auto 0;border-radius:8em;background-color:#121212;cursor:pointer;transition:color 0.8s linear,background 0.8s linear,box-shadow 0.1s linear,transform 0.1s linear}div{display:block;position:relative;top:50%;height:16em;width:16em;margin:-8em auto 0;border-radius:8em;background-color:#121212;cursor:pointer;transition:color 0.8s linear,background 0.8s linear,box-shadow 0.1s linear,transform 0.1s linear}.codepen{display:block;position:relative;top:50%;height:16em;width:16em;margin:-8em auto 0;border-radius:8em;background-color:#121212;cursor:pointer;transition:color 0.8s linear,background 0.8s linear,box-shadow 0.1s linear,transform 0.1s linear}*{box-sizing:border-box}body{background:#222;font-family:'Gochi Hand',sans-serif;color:#333}aside.context{text-align:center;color:#fff;line-height:1.7;font-size:20px;letter-spacing:.5px}.context{text-align:center;color:#fff;line-height:1.7;font-size:20px;letter-spacing:.5px}aside{text-align:center;color:#fff;line-height:1.7;font-size:20px;letter-spacing:.5px}a{text-decoration:none;color:#fff;padding:3px 0;border-bottom:1px dashed}a:hover{border-bottom:1px solid}footer{text-align:center;margin:4em auto;width:100%}footer a{text-decoration:none;display:inline-block;width:45px;height:45px;border-radius:50%;background:transparent;border:1px dashed #fff;color:#fff;margin:5px}a{text-decoration:none;display:inline-block;width:45px;height:45px;border-radius:50%;background:transparent;border:1px dashed #fff;color:#fff;margin:5px}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHvOiQeRryW0"
      },
      "source": [
        "Tenemos alrededor de 1 millón de carácteres en nuestro dataset, suficientes para generar texto de manera convincente como si fuésemos el manco de Lepanto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKi9CXqzryW0"
      },
      "source": [
        "## Tokenización"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKhnGrreryW1"
      },
      "source": [
        "Para poder darle este texto a nuestra red neuronal necesitamos transformarlo en números con los que podemos llevar a cabo las operaciones que tienen lugar en la red. Este proceso se conoce como `tokenización`. Existen muchas formas de llevar a cabo este proceso, en este caso simplemente sustituiremos cada carácter en nuestro texto por su posición en el siguiente vector de carácteres."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-31T14:37:39.453968Z",
          "start_time": "2020-08-31T14:37:39.440969Z"
        },
        "id": "YgRUgWAdryW2",
        "outputId": "3a19449c-e259-4c3f-cf37-54e75fa347c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0cñÑáÁéÉíÍóÓúÚ¿¡'"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import string\n",
        "\n",
        "all_characters = string.printable + \"ñÑáÁéÉíÍóÓúÚ¿¡\"\n",
        "all_characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-31T14:37:39.469968Z",
          "start_time": "2020-08-31T14:37:39.454970Z"
        },
        "id": "GSFY8Uk5ryW2",
        "outputId": "3d3486bd-be02-4d30-c497-b5ddfbb63203",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "114"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import string\n",
        "\n",
        "class Tokenizer(): \n",
        "    \n",
        "  def __init__(self):\n",
        "    self.all_characters = all_characters\n",
        "    self.n_characters = len(self.all_characters)\n",
        "    \n",
        "  def text_to_seq(self, string):\n",
        "    seq = []\n",
        "    for c in range(len(string)):\n",
        "        try:\n",
        "            seq.append(self.all_characters.index(string[c]))\n",
        "        except:\n",
        "            continue\n",
        "    return seq\n",
        "\n",
        "  def seq_to_text(self, seq):\n",
        "    text = ''\n",
        "    for c in range(len(seq)):\n",
        "        text += self.all_characters[seq[c]]\n",
        "    return text\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.n_characters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6IY38C7ryW4"
      },
      "source": [
        "El tokenizer puede convertir una secuencia de texto en números, y al revés."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-31T14:37:39.485968Z",
          "start_time": "2020-08-31T14:37:39.470969Z"
        },
        "id": "bFamVk6zryW4",
        "outputId": "fdd26221-6563-49fb-8fb1-71e2f6bbfc62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[28, 14, 100, 24, 27, 73, 94, 112, 26, 30, 104, 94, 29, 10, 21, 82]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "tokenizer.text_to_seq('señor, ¿qué tal?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-31T14:37:39.501968Z",
          "start_time": "2020-08-31T14:37:39.486970Z"
        },
        "id": "YKHdVVncryW5",
        "outputId": "3ad47327-9904-40c4-ef31-1a1f24a98dfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'señor, ¿qué tal?'"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "tokenizer.seq_to_text([28, 14, 100, 24, 27, 73, 94, 112, 26, 30, 104, 94, 29, 10, 21, 82])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSshh7ELryW6"
      },
      "source": [
        "Ahora podemos tokenizar todo el texto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-31T14:37:39.756970Z",
          "start_time": "2020-08-31T14:37:39.503970Z"
        },
        "id": "wiw86qnpryW6"
      },
      "outputs": [],
      "source": [
        "text_encoded = tokenizer.text_to_seq(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wwFUFEZryW6"
      },
      "source": [
        "> 💡 Pese a que podemos implementar nuestra lógica de tokenización para trabajar a nivel de carácteres, cuando trabajamos con palabras completas el proceso puede complicarse. Es por esto que existen muchas herramientas que ya implementan este tipo de procesado (y muchos otros) que podemos utilizar. Un ejemplo, especialmente integrado con `Pytorch`, es la librería [torchtext](https://pytorch.org/text/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO6-PWcTryW7"
      },
      "source": [
        "## El *Dataset*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbuIxrAXryW7"
      },
      "source": [
        "En primer lugar, vamos a separar nuestro texto en un conjunto de entrenamiento y otro de test. Cómo ya hemos hablado en posts anteriores, usaremos los datos de entrenamiento para entrenar nuestra red neuronal y los datos de test para calcular las métricas finales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-31T14:37:39.772968Z",
          "start_time": "2020-08-31T14:37:39.757969Z"
        },
        "id": "NjwBghoSryW7",
        "outputId": "e458d79f-53a0-41fb-dcd7-56f9d0345350",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6768, 1693)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "train_size = len(text_encoded) * 80 // 100 \n",
        "train = text_encoded[:train_size]\n",
        "test = text_encoded[train_size:]\n",
        "\n",
        "len(train), len(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZOeckQJryW8"
      },
      "source": [
        "Para entrenar nuestra red, vamos a necesitar secuencias de texto de una longitud determinada. Podemos generar estas ventanas con la siguiente función"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-31T14:37:42.399028Z",
          "start_time": "2020-08-31T14:37:39.773969Z"
        },
        "id": "6VODvt4iryW8"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def windows(text, window_size = 100):\n",
        "    start_index = 0\n",
        "    end_index = len(text) - window_size\n",
        "    text_windows = []\n",
        "    while start_index < end_index:\n",
        "      text_windows.append(text[start_index:start_index+window_size+1])\n",
        "      start_index += 1\n",
        "    return text_windows\n",
        "\n",
        "text_encoded_windows = windows(text_encoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3QA4OKXryW9"
      },
      "source": [
        "Como puedes ver, hemos generado un número determinado de frases con la longitud especificada las cuales empiezan cada vez un carácter más a la derecha."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-31T14:37:42.415028Z",
          "start_time": "2020-08-31T14:37:42.400029Z"
        },
        "id": "WZHGSNU-ryW9",
        "outputId": "d97ce291-a672-41b4-b782-77b643ba8cc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "body{overflow:hidden;background:rgb(25,35,125)}div.drop-container{position:absolute;top:0;right:0;bot\n",
            "\n",
            "ody{overflow:hidden;background:rgb(25,35,125)}div.drop-container{position:absolute;top:0;right:0;bott\n",
            "\n",
            "dy{overflow:hidden;background:rgb(25,35,125)}div.drop-container{position:absolute;top:0;right:0;botto\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.seq_to_text((text_encoded_windows[0])))\n",
        "print()\n",
        "print(tokenizer.seq_to_text((text_encoded_windows[1])))\n",
        "print()\n",
        "print(tokenizer.seq_to_text((text_encoded_windows[2])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbdcqThmryW_"
      },
      "source": [
        "Nuestro *dataset* de `Pytorch` se encargará de darnos cada una de estas frases, utilizando todos los carácteres excepto el último como entradas para la red y el último carácter como la etiqueta que usaremos durante el entrenamiento (la red deberá predecir la siguiente letra)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-31T14:37:43.505162Z",
          "start_time": "2020-08-31T14:37:42.416029Z"
        },
        "id": "Ix_P-4yuryXA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class CharRNNDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, text_encoded_windows, train=True):\n",
        "    self.text = text_encoded_windows\n",
        "    self.train = train\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.text)\n",
        "\n",
        "  def __getitem__(self, ix):\n",
        "    if self.train:\n",
        "      return torch.tensor(self.text[ix][:-1]), torch.tensor(self.text[ix][-1])\n",
        "    return torch.tensor(self.text[ix])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-31T14:37:46.441198Z",
          "start_time": "2020-08-31T14:37:43.506164Z"
        },
        "id": "SFIFbMZzryXA",
        "outputId": "20129cfc-f45a-44ea-f3f3-aab7482baa60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6668, 1593)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "train_text_encoded_windows = windows(train)\n",
        "test_text_encoded_windows = windows(test)\n",
        "\n",
        "dataset = {\n",
        "    'train': CharRNNDataset(train_text_encoded_windows),\n",
        "    'val': CharRNNDataset(test_text_encoded_windows)\n",
        "}\n",
        "\n",
        "dataloader = {\n",
        "    'train': torch.utils.data.DataLoader(dataset['train'], batch_size=512, shuffle=True, pin_memory=True),\n",
        "    'val': torch.utils.data.DataLoader(dataset['val'], batch_size=2048, shuffle=False, pin_memory=True),\n",
        "}\n",
        "\n",
        "len(dataset['train']), len(dataset['val'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-31T14:37:46.472195Z",
          "start_time": "2020-08-31T14:37:46.443197Z"
        },
        "id": "gsKPzp7UryXB",
        "outputId": "fca763a2-c5c1-48dc-cc61-a86b90661578",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'body{overflow:hidden;background:rgb(25,35,125)}div.drop-container{position:absolute;top:0;right:0;bo'"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "input, output = dataset['train'][0]\n",
        "tokenizer.seq_to_text(input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-31T14:37:46.488195Z",
          "start_time": "2020-08-31T14:37:46.473196Z"
        },
        "id": "NnbVkEDSryXB",
        "outputId": "ad0ff2e6-62f0-4026-8bc8-6b45b1765df1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'t'"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "tokenizer.seq_to_text([output])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pCV5xGzryXC"
      },
      "source": [
        "## Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwgSbkC5ryXC"
      },
      "source": [
        "Si bien hemos conseguido convertir nuestro texto a números, una red neuronal seguirá sin ser capaz de trabajar con nuestros datos ya que, como hemos visto en posts anteriores, éstos tienen que estar normalizados. Además, en función del `tokenizador` que utilicemos es posible que el  mismo carácter tenga asociados diferentes valores. Es por esto que necesitamos codificar nuestro texto de alguna manera. \n",
        "\n",
        "Una opción puede ser el `one-hot encoding`, al fin y al cabo podemos considerar cada letra como una categoría y que nuestra red nos de a la salida una distribución de probabilidad sobre todos los posibles carácteres. A continuación tienes un ejemplo de este tipo de codificación (utilizando palabras en vez de letras).\n",
        "\n",
        "![](https://i0.wp.com/shanelynnwebsite-mid9n9g1q9y8tt.netdna-ssl.com/wp-content/uploads/2018/01/one-hot-word-embedding-vectors.png?ssl=1)\n",
        "\n",
        "A nuestra red le daremos a la entrada un vector que representará cada elemento en el vocabulario. Este vector tendrá una longitud igual al número de elementos diferentes en el vocabulario, y estará lleno de ceros excepto por una posición (la posición que ocupe el elemento en concreto dentro del vocabulario, la lista de elementos únicos). En nuestro caso podríamos optar por esta alternativa, ya que apenas tenemos un centenar de carácteres diferentes. Sin embargo, cuando trabajemos con palabras, nuestros vocabularios serán enormes (¿cuántas palabras hay en el diccionario?). Esto implica que trabajar con una codificación `one-hot` será muy costoso (vectores muy grandes) e ineficiente (prácticamente llenos de ceros). Es por esto que utilizamos una mejor codificación: los `embeddings`\n",
        "\n",
        "![](https://i.stack.imgur.com/5gAnY.png)\n",
        "\n",
        "Un embedding es una matriz con un número de filas igual al tamaño del vocabulario y un número de columnas que nosotros decidiremos. Cada fila en la matriz representará la codificación de una palabara (o carácter en nuestro ejemplo). A diferencia de la codificación `one-hot`, estos vectores son densos (pueden tener valores diferentes de cero en cualquier posición). Además, estos valores son aprendidos por la red neuronal, de manera que podrá representar los datos de la mejor forma posible para llevar a cabo la tarea. En la figura anterior tienes un ejemplo de un embedding entrenado, ¿observas algún patrón?. Efectivamente, palabras similares tienen representaciones similares. Además, cada columna del embedding tiene un significado que permite establecer relaciones entre las diferentes representaciones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kePru7TdryXD"
      },
      "source": [
        "> ⚡ ¿Qué resultado obtienes al restar el vector `boy` al vector `man` y sumarle el vector `girl`?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBZ7lGw-ryXD"
      },
      "source": [
        "En `Pytorch` tenemos esta capa implementada en la clase `torch.nn.Embedding`, y más adelante veremos como podemos utilizar `transfer learning` con embeddings pre-entrenados (lo cual nos dará una mejor representación de nuestro vocabulario desde el principio sin tener que entrenar esta capa)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-31T14:37:46.504195Z",
          "start_time": "2020-08-31T14:37:46.489196Z"
        },
        "id": "ztrYNqphryXD"
      },
      "outputs": [],
      "source": [
        "class CharRNN(torch.nn.Module):\n",
        "  def __init__(self, input_size, embedding_size=128, hidden_size=256, num_layers=2, dropout=0.2):\n",
        "    super().__init__()\n",
        "    self.encoder = torch.nn.Embedding(input_size, embedding_size)\n",
        "    self.rnn = torch.nn.LSTM(input_size=embedding_size, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout, batch_first=True)\n",
        "    self.fc = torch.nn.Linear(hidden_size, input_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.encoder(x)\n",
        "    x, h = self.rnn(x)         \n",
        "    y = self.fc(x[:,-1,:])\n",
        "    return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_C2UA6InryXD"
      },
      "source": [
        "Nuestro modelo recibirá *batches* de frases con el índice de cada palabra que nos proporciona el `tokenizador`. A la salida tendremos una distribución de probabilidad sobre todos los posibles carácteres para cada frase del *batch*. Aquellos con mayor probabilidad serán los que la red cree que son buenos candidatos para seguir la frase recibida a la entrada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-31T14:37:46.647195Z",
          "start_time": "2020-08-31T14:37:46.505196Z"
        },
        "id": "99ILHDEkryXD",
        "outputId": "bbaeae79-a097-4e93-b872-fc1e55504291",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 114])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "model = CharRNN(input_size=tokenizer.n_characters)\n",
        "outputs = model(torch.randint(0, tokenizer.n_characters, (64, 50)))\n",
        "outputs.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVWppEtIryXE"
      },
      "source": [
        "## Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-31T14:37:46.709195Z",
          "start_time": "2020-08-31T14:37:46.649195Z"
        },
        "code_folding": [
          37
        ],
        "id": "Q2cjcnGTryXE"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def fit(model, dataloader, epochs=10):\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    for epoch in range(1, epochs+1):\n",
        "        model.train()\n",
        "        train_loss = []\n",
        "        bar = tqdm(dataloader['train'])\n",
        "        for batch in bar:\n",
        "            X, y = batch\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            y_hat = model(X)\n",
        "            loss = criterion(y_hat, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss.append(loss.item())\n",
        "            bar.set_description(f\"loss {np.mean(train_loss):.5f}\")\n",
        "        bar = tqdm(dataloader['val'])\n",
        "        val_loss = []\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch in bar:\n",
        "                X, y = batch\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                y_hat = model(X)\n",
        "                loss = criterion(y_hat, y)\n",
        "                val_loss.append(loss.item())\n",
        "                bar.set_description(f\"val_loss {np.mean(val_loss):.5f}\")\n",
        "        print(f\"Epoch {epoch}/{epochs} loss {np.mean(train_loss):.5f} val_loss {np.mean(val_loss):.5f}\")\n",
        "\n",
        "def predict(model, X):\n",
        "    model.eval() \n",
        "    with torch.no_grad():\n",
        "        X = torch.tensor(X).to(device)\n",
        "        pred = model(X.unsqueeze(0))\n",
        "        return pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-31T15:43:20.416599Z",
          "start_time": "2020-08-31T14:37:46.711195Z"
        },
        "id": "qLIg8KIAryXE",
        "outputId": "22785765-5475-4cc6-cbdb-ac63eaacf0da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 4.12709: 100%|██████████| 14/14 [00:03<00:00,  4.32it/s]\n",
            "val_loss 3.56558: 100%|██████████| 1/1 [00:00<00:00,  4.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 loss 4.12709 val_loss 3.56558\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 3.62938: 100%|██████████| 14/14 [00:02<00:00,  4.90it/s]\n",
            "val_loss 3.49803: 100%|██████████| 1/1 [00:00<00:00,  5.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 loss 3.62938 val_loss 3.49803\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 3.53404: 100%|██████████| 14/14 [00:02<00:00,  4.89it/s]\n",
            "val_loss 3.43233: 100%|██████████| 1/1 [00:00<00:00,  5.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20 loss 3.53404 val_loss 3.43233\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 3.41346: 100%|██████████| 14/14 [00:02<00:00,  4.93it/s]\n",
            "val_loss 3.31007: 100%|██████████| 1/1 [00:00<00:00,  4.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/20 loss 3.41346 val_loss 3.31007\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 3.27826: 100%|██████████| 14/14 [00:02<00:00,  4.88it/s]\n",
            "val_loss 3.19386: 100%|██████████| 1/1 [00:00<00:00,  5.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/20 loss 3.27826 val_loss 3.19386\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 3.10290: 100%|██████████| 14/14 [00:02<00:00,  4.89it/s]\n",
            "val_loss 3.03029: 100%|██████████| 1/1 [00:00<00:00,  5.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/20 loss 3.10290 val_loss 3.03029\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 2.97384: 100%|██████████| 14/14 [00:02<00:00,  4.88it/s]\n",
            "val_loss 2.93278: 100%|██████████| 1/1 [00:00<00:00,  5.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/20 loss 2.97384 val_loss 2.93278\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 2.88999: 100%|██████████| 14/14 [00:02<00:00,  4.86it/s]\n",
            "val_loss 2.80404: 100%|██████████| 1/1 [00:00<00:00,  5.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/20 loss 2.88999 val_loss 2.80404\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 2.71375: 100%|██████████| 14/14 [00:02<00:00,  4.81it/s]\n",
            "val_loss 2.67035: 100%|██████████| 1/1 [00:00<00:00,  5.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/20 loss 2.71375 val_loss 2.67035\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 2.59708: 100%|██████████| 14/14 [00:02<00:00,  4.82it/s]\n",
            "val_loss 2.57032: 100%|██████████| 1/1 [00:00<00:00,  4.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/20 loss 2.59708 val_loss 2.57032\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 2.46844: 100%|██████████| 14/14 [00:02<00:00,  4.84it/s]\n",
            "val_loss 2.45509: 100%|██████████| 1/1 [00:00<00:00,  5.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/20 loss 2.46844 val_loss 2.45509\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 2.39400: 100%|██████████| 14/14 [00:02<00:00,  4.82it/s]\n",
            "val_loss 2.34988: 100%|██████████| 1/1 [00:00<00:00,  5.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/20 loss 2.39400 val_loss 2.34988\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 2.27317: 100%|██████████| 14/14 [00:02<00:00,  4.82it/s]\n",
            "val_loss 2.28788: 100%|██████████| 1/1 [00:00<00:00,  5.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/20 loss 2.27317 val_loss 2.28788\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 2.13510: 100%|██████████| 14/14 [00:02<00:00,  4.75it/s]\n",
            "val_loss 2.18354: 100%|██████████| 1/1 [00:00<00:00,  5.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/20 loss 2.13510 val_loss 2.18354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 2.05080: 100%|██████████| 14/14 [00:02<00:00,  4.81it/s]\n",
            "val_loss 2.13729: 100%|██████████| 1/1 [00:00<00:00,  3.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/20 loss 2.05080 val_loss 2.13729\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.93452: 100%|██████████| 14/14 [00:02<00:00,  4.80it/s]\n",
            "val_loss 2.04973: 100%|██████████| 1/1 [00:00<00:00,  4.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/20 loss 1.93452 val_loss 2.04973\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.80182: 100%|██████████| 14/14 [00:02<00:00,  4.80it/s]\n",
            "val_loss 1.98123: 100%|██████████| 1/1 [00:00<00:00,  4.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/20 loss 1.80182 val_loss 1.98123\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.71380: 100%|██████████| 14/14 [00:02<00:00,  4.75it/s]\n",
            "val_loss 1.92471: 100%|██████████| 1/1 [00:00<00:00,  4.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/20 loss 1.71380 val_loss 1.92471\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.62674: 100%|██████████| 14/14 [00:02<00:00,  4.77it/s]\n",
            "val_loss 1.88185: 100%|██████████| 1/1 [00:00<00:00,  5.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/20 loss 1.62674 val_loss 1.88185\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loss 1.56245: 100%|██████████| 14/14 [00:02<00:00,  4.71it/s]\n",
            "val_loss 1.82488: 100%|██████████| 1/1 [00:00<00:00,  5.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/20 loss 1.56245 val_loss 1.82488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "model = CharRNN(input_size=tokenizer.n_characters)\n",
        "fit(model, dataloader, epochs=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGIfg6X9ryXF"
      },
      "source": [
        "## Generando texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqhgmL3sryXF"
      },
      "source": [
        "Una vez hemos entrenado nuestro modelo, podemos darle una frase para que genere la siguiente letra."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-31T15:43:20.431602Z",
          "start_time": "2020-08-31T15:43:20.417599Z"
        },
        "id": "oB2-PKocryXF",
        "outputId": "31a6bc86-4036-4c49-adb9-f26e8a4cd214",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'m'"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "X_new = \"body{\"\n",
        "X_new_encoded = tokenizer.text_to_seq(X_new)\n",
        "y_pred = predict(model, X_new_encoded)\n",
        "y_pred = torch.argmax(y_pred, axis=1)[0].item()\n",
        "tokenizer.seq_to_text([y_pred])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Twsqw6fWryXF"
      },
      "source": [
        "Podemos generar más letras añadiendo las predicciones como parte de la entrada, generando texto letra a letra."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-31T15:43:20.653601Z",
          "start_time": "2020-08-31T15:43:20.433602Z"
        },
        "id": "0rN1_oSCryXF",
        "outputId": "e61df60d-5ec4-4e6f-a7ad-b0927505b511",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'body{margin:1;transform:cale(-50% - 25px);border-radiun:1;margin:1;transform:cale(-50% - 25px);border-radiun:1;margin:1;transform:cale(-50% - 25px);border-radiun:1;margin:1;transform:cale(-50% - 25px);border-radiun:1;margin:1;transform:cale(-50% - 25px);border-radiun:1;margin:1;transform:cale(-50% - 25px'"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "for i in range(100):\n",
        "  X_new_encoded = tokenizer.text_to_seq(X_new[-100:])\n",
        "  y_pred = predict(model, X_new_encoded)\n",
        "  y_pred = torch.argmax(y_pred, axis=1)[0].item()\n",
        "  X_new += tokenizer.seq_to_text([y_pred])\n",
        "\n",
        "X_new"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26aMlJCxryXF"
      },
      "source": [
        "Cómo puedes ver el text generado puede ser repetitivo si simplemente nos quedamos con la letra con mayor probabilidad. Para generar texto con mayor variedad, es común elegir de manera aleatoria una letra de entre las que tienen mayor probabilidad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-31T15:59:01.326415Z",
          "start_time": "2020-08-31T15:58:58.658570Z"
        },
        "id": "NSVaYCF1ryXG",
        "outputId": "811f1a64-3f40-4517-93da-b34202b22d95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "body{margin:1;transform:cale(-50% - 25px);border-radiun:1;margin:1;transform:cale(-50% - 25px);border-radiun:1;margin:1;transform:cale(-50% - 25px);border-radiun:1;margin:1;transform:cale(-50% - 25px);border-radiun:1;margin:1;transform:cale(-50% - 25px);border-radiun:1;margin:1;transform:cale(-50% - 25px 210%}byht:iverhe 3s e(,ansity-rhebtcole:';zit-:calvc(0+9 lhc(1400  B'mÓ al(ac(lec(}zniteht:.5em50%;min-}ht-txan-seah-bolor:0ralyalh{m}ationo:.5{p}adkion:bow;let:2contemtracitent:1;banov-re-rader:0;posirifh-sitem:1;margin:}p:6c;derifttr{fort:0;pagngin:ttop:fixem;orex-tifteom:#ff;borkilatcfolce.nett;tixt-saty:2#5o#7.5mprian-coshtion-gimein:imentrayelhy1# sfffop:#35;bocko{tnanito{w-site:fopx;.js-sime:17;coms:1 0px;outton-oreatifon:.cex;wiith:lux;findexid-lation:cofornt-sraderean;sopady-le:fang,an-ciopne 0a3s o'tox:1;targhs2{1emtbith:4.v;lh-sizw:cote;grid:robe;height:.5px;tont--siation:heig;tiane:cele :atentolftefoc-oletn:6}purdips{pad-inggw:#-0}leftsan-peadis efon:camail{top:.5em;clorn:celtectemlaniwigp:0 em}-cexte-ftrosit-iliun:4fe-co9w;bowit:'85}x;rods-witk-ttop:0*+ absjov-ar(-ru);backdent-ramit:1;ation-)in:v(,raciux;zrd-rosi:s^eile;fheythatiyutyh:60%;dorew-anitewti-size:Hoor(-{lemotef:cegconofO-lez 0anite;ly:claner;(artign:.Tlen;rrads-tpansite.tent-s-xe;igto:{wigbhh:hed;lor:10r}v #-2f\n"
          ]
        }
      ],
      "source": [
        "temp=1\n",
        "for i in range(1000):\n",
        "  X_new_encoded = tokenizer.text_to_seq(X_new[-100:])\n",
        "  y_pred = predict(model, X_new_encoded)\n",
        "  y_pred = y_pred.view(-1).div(temp).exp()\n",
        "  top_i = torch.multinomial(y_pred, 1)[0]\n",
        "  predicted_char = tokenizer.all_characters[top_i]\n",
        "  X_new += predicted_char\n",
        "\n",
        "print(X_new)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6ObqH92ryXG"
      },
      "source": [
        "## Resumen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0rybR-fryXG"
      },
      "source": [
        "En este post hemos aprendido cómo implementar y entrenar una `red neuronal recurrente` para generar texto como si fuese Miguel de Cervantes. Para ello hemos utilizado su libro *Don Quijote de la Mancha* como dataset. En primer lugar, transformamos el texto en números gracias al proceso de la `tokenización`. Después, codificamos cada carácter en el dataset utilizando una capa `embedding`, que permitirá a la red neuronal encontrar la mejor representación posible de los datos para llevar a cabo su tarea. Para generar texto, le pedimos a la red que nos de una distribución de probabilidad sobre todos los posible carácteres a partir de una frase que le damos a la entrada. Utilizaremos esta distribución para seleccionar un carácter que siga con la frase de manera convincente. Podemos repetir este proceso para generar secuencias más largas."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "233.594px"
      },
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "PruebaRNN.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}