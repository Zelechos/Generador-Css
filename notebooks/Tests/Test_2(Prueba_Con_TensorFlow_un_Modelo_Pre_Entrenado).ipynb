{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Test_2(Prueba Con TensorFlow un Modelo Pre Entrenado).ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zelechos/Generador-Css/blob/main/notebooks/Tests/Test_2(Prueba_Con_TensorFlow_un_Modelo_Pre_Entrenado).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Inspirado en este video](https://www.youtube.com/watch?v=p2sTJYoIwj0)"
      ],
      "metadata": {
        "id": "ko91psEeb12R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "k0zFXfqjbhFf",
        "outputId": "e9f510b3-d653-4fd2-d6bf-91a4c340c16f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Generador-Css'...\n",
            "remote: Enumerating objects: 193, done.\u001b[K\n",
            "remote: Counting objects: 100% (193/193), done.\u001b[K\n",
            "remote: Compressing objects: 100% (164/164), done.\u001b[K\n",
            "remote: Total 193 (delta 85), reused 78 (delta 24), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (193/193), 2.06 MiB | 10.73 MiB/s, done.\n",
            "Resolving deltas: 100% (85/85), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Zelechos/Generador-Css.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instalamos el modelo preentrenado para hacer pruebas"
      ],
      "metadata": {
        "id": "MNAISa9hdrEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-transformer"
      ],
      "metadata": {
        "id": "qXggzuWZdO-s",
        "outputId": "12707e1e-c696-4c5a-c709-59148b077824",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-transformer\n",
            "  Downloading keras-transformer-0.39.0.tar.gz (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-transformer) (1.19.5)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.7/dist-packages (from keras-transformer) (2.7.0)\n",
            "Collecting keras-pos-embd>=0.12.0\n",
            "  Downloading keras-pos-embd-0.12.0.tar.gz (6.0 kB)\n",
            "Collecting keras-multi-head>=0.28.0\n",
            "  Downloading keras-multi-head-0.28.0.tar.gz (14 kB)\n",
            "Collecting keras-layer-normalization>=0.15.0\n",
            "  Downloading keras-layer-normalization-0.15.0.tar.gz (4.2 kB)\n",
            "Collecting keras-position-wise-feed-forward>=0.7.0\n",
            "  Downloading keras-position-wise-feed-forward-0.7.0.tar.gz (4.5 kB)\n",
            "Collecting keras-embed-sim>=0.9.0\n",
            "  Downloading keras-embed-sim-0.9.0.tar.gz (4.1 kB)\n",
            "Collecting keras-self-attention>=0.50.0\n",
            "  Downloading keras-self-attention-0.50.0.tar.gz (12 kB)\n",
            "Building wheels for collected packages: keras-transformer, keras-embed-sim, keras-layer-normalization, keras-multi-head, keras-pos-embd, keras-position-wise-feed-forward, keras-self-attention\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.39.0-py3-none-any.whl size=12842 sha256=3d118e9d0e7cbc9d371cb1806c9daa10fffacaf8b2f34d9840b5e273a2926a10\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/01/e0/5a1a14bed6726f2ed73f7917d2d2c2d4081d2c88426dea07ce\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.9.0-py3-none-any.whl size=4504 sha256=e43bc830d09f767cdb6bac802764930b3d817291135ffdf881fdab75a2945830\n",
            "  Stored in directory: /root/.cache/pip/wheels/a8/1e/d2/9bc15513dd2f8b9de3e628b3aa9d2de49e721deef6bbd1497e\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.15.0-py3-none-any.whl size=5224 sha256=394c00c197c9c616c1b858e5a2a3838b3e0212cf07a45bd3d0c911fea9f95316\n",
            "  Stored in directory: /root/.cache/pip/wheels/4d/be/fe/55422f77ac11fe6ddcb471198038de8a26b5a4dd1557883c1e\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.28.0-py3-none-any.whl size=15559 sha256=3b7014ac26c5b41eb5564eead2b2685edf987e13340226efb5f1370e8251dbca\n",
            "  Stored in directory: /root/.cache/pip/wheels/79/4a/ea/9503ab5a02201dfb8635ba2cc8f30844661623c684a5b44472\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.12.0-py3-none-any.whl size=7469 sha256=bb34994d687824dfde8dc264e43aab135058490c84f0117290b4f5f20cf95325\n",
            "  Stored in directory: /root/.cache/pip/wheels/77/99/fd/dd98f4876c3ebbef7aab0dbfbd37bca41d7db37d3a28b2cb09\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.7.0-py3-none-any.whl size=5541 sha256=1a06738668e0b895a7cbf458bed71b1f8f4c236d2ca233a9c7a7600861910e41\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/12/02/1ad455c4f181cda1a4e60c5445855853d5c2ea91f942586a04\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.50.0-py3-none-any.whl size=19414 sha256=651bb003c769bb017ea5365c7f31f58e3a63b08769951e55b33ec4ff7cb8cbd4\n",
            "  Stored in directory: /root/.cache/pip/wheels/92/7a/a3/231bef5803298e7ec1815215bc0613239cb1e9c03c57b13c14\n",
            "Successfully built keras-transformer keras-embed-sim keras-layer-normalization keras-multi-head keras-pos-embd keras-position-wise-feed-forward keras-self-attention\n",
            "Installing collected packages: keras-self-attention, keras-position-wise-feed-forward, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-embed-sim, keras-transformer\n",
            "Successfully installed keras-embed-sim-0.9.0 keras-layer-normalization-0.15.0 keras-multi-head-0.28.0 keras-pos-embd-0.12.0 keras-position-wise-feed-forward-0.7.0 keras-self-attention-0.50.0 keras-transformer-0.39.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras_transformer import get_model, decode\n",
        "np.random.seed(0)"
      ],
      "metadata": {
        "id": "MT7630sTdxID"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Iniciamos con el preprocesamiento de los datos"
      ],
      "metadata": {
        "id": "NfHF8XDtea-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open('/content/Generador-Css/dataset/TEST.json') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "#accedemos a la los datos\n",
        "data_styles = data[\"styles\"]\n",
        "print(data_styles[0])\n",
        "\n",
        "\n",
        "#accediendo a los estilos\n",
        "styles = data_styles[0][\"style\"]\n",
        "print(\"\\nstyles access => \",styles)\n",
        "\n",
        "\n",
        "#accediendo a los selectores\n",
        "selector = data_styles[0][\"selector\"]\n",
        "print(\"\\nselector access => \",selector)"
      ],
      "metadata": {
        "id": "Ua8bzyK7dBQ2",
        "outputId": "3fbc6c72-10f6-40fd-aaca-da3e1f49edb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'style': 'body { overflow : hidden ; background : rgb(25,35,125) } ', 'selector': 'body'}\n",
            "\n",
            "styles access =>  body { overflow : hidden ; background : rgb(25,35,125) } \n",
            "\n",
            "selector access =>  body\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_css = []\n",
        "styles = []\n",
        "selectors = []\n",
        "for data in data_styles:\n",
        "  selectors.append(data[\"selector\"])\n",
        "  styles.append(data[\"style\"])\n",
        " \n",
        "# print(selectors)\n",
        "# print(styles)\n",
        "data_css.append(selectors)\n",
        "data_css.append(styles)\n",
        "print(data_css[0])\n",
        "print(data_css[1])"
      ],
      "metadata": {
        "id": "mED1G4PzdCBu",
        "outputId": "875c9826-3eba-4b67-b936-51d4ba3c7ca8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['body', 'div.drop-container', 'div.drop', 'h1', '@font-face', 'body', '.logo', '.switch-left', '.pad-left', '.switch-right', '.pad-right', '.line', '.text', '.text', '.text', '.reload', '.logo', 'html,body', 'body', '*', '*', '*', '.css_lab', '.witch', 'body', 'body img', 'img', 'body h1', 'h1', 'body a', 'a', '@keyframes glow', 'from', 'to', 'body', 'h1', 'ul', 'li', 'li : hover span', 'li.invisible', 'li.animate', 'span', 'span : before', 'html', 'body', 'header', 'article', 'h3,section', 'h3', 'section', 'h3 : before', 'h3 : before,section : before', 'section : before', 'h3', 'section', 'h3 : before', 'section : before', 'p', 'a', 'body,html', 'body', 'html', 'div.codepen', 'div', '.codepen', '*', 'body', 'aside.context', '.context', 'aside', 'a', 'a : hover', 'footer', 'footer a', 'a', 'body', 'p', '.img-centered', '.bg-light-gray', '.bg-darkest-gray', '.btn-primary', '.navbar-default', '.navbar-default.navbar-brand', '.navbar-default.navbar-collapse', '.navbar-default.nav li a', '.nav li a', '.button', '.button', '.button', '.button']\n",
            "['body { overflow : hidden ; background : rgb(25,35,125) } ', 'div.drop-container { position : absolute ; top : 0 ; right : 0 ; bottom : 0 ; left : 0 ; margin : auto ; height : 200px ; width : 200px } ', 'div.drop { position : absolute ; top : -25% ; width : 100% ; height : 100% ; border-radius : 100% 5% 100% 100% ; -webkit-transform : rotate(-45deg) ; transform : rotate(-45deg) ; margin : 0 ; background : deepskyblue ; -webkit-animation : drip 4s forwards ; animation : drip 4s forwards } ', 'h1 { color : #fff ; position : absolute ; font-size : 2.5em ; height : 1em ; top : 0 ; left : 0 ; right : 0 ; bottom : 0 ; z-index : 2 ; margin : auto ; text-align : center ; opacity : 0 ; -webkit-animation : appear 2s 2.5s forwards ; animation : appear 2s 2.5s forwards } ', '@font-face  { font-family :  logofont ; src :  url(https : //s3-us-west-2.amazonaws.com/s.cdpn.io/531144/couture-bld.otf) ; } ', 'body { margin :  0px ; background :  #EF5350 ; } ', '.logo { transform :  scale(0.7) ; margin-top :  45px ; } ', '.switch-left { position : fixed ; background : transparent ; width : 130px ; height : 380px ; border-radius : 100px 5px 5px 100px ; border : 35px solid #fff ; transform : translate(calc(50vw - 217px),calc(50vh - 225px)) ; animation : switch-left-animation 1.75s ease } ', '.pad-left { position : fixed ; width : 80px ; height : 80px ; background : #fff ; border-radius : 100% ; margin-top : 45px ; margin-left : 27.5px } ', '.switch-right { position : fixed ; background : #fff ; width : 185px ; height : 450px ; border-radius : 5px 100px 100px 5px ; transform : translate(calc(50vw + 17px),calc(50vh - 225px)) ; animation : switch-right-animation 1.75s ease } ', '.pad-right { position : fixed ; width : 80px ; height : 80px ; background : #EF5350 ; border-radius : 100% ; margin-top : 225px ; margin-left : 50px } ', '.line { width : 6px ; height : 100vh ; position : fixed ; background : #fff ; top : 0 ; left : calc(50vw - 3px) ; opacity : .25 } ', '.text { font-family : logofont ; font-size : 80px ; color : #fff ; letter-spacing : 19px ; position : fixed ; transform : translate(calc(50vw - 288px),calc(50vh + 175px)) } ', '.text { font-family : logofont ; font-size : 124px ; color : #fff ; letter-spacing : 13px ; position : fixed ; transform : translate(calc(50vw - 290px),calc(50vh + 200px)) } ', '.text { animation :  text-animation 1.75s ease ; position :  fixed ; } ', '.reload { position : fixed ; width : 75px ; height : 75px ; border-radius : 5px ; left : calc(50vw - 37.5px) ; bottom : calc(50vh - 345px) ; cursor : pointer ; transition : background 0.3s ease ; animation : reload-animation 2.5s ease } ', ' .logo { transform :  scale(0.45) ; margin-top :  65px ; } ', 'html,body { margin : 0 ; padding : 0 ; width : 100% ; height : 100% ; overflow : hidden ; } ', 'body { margin : 0 ; padding : 0 ; width : 100% ; height : 100% ; overflow : hidden ; } ', '* { margin : 0 ; padding : 0 } ', '* { margin : 0 ; padding : 0 ; box-sizing : border-box ; } ', '* { box-sizing : border-box ; } ', '.css_lab { width : 100% ; height : 100% ; background : #224 } ', '.witch { width : 100px ; height : 100px ; position : absolute ; top : 50% ; left : 50% ; margin : -60px ; -webkit-transform : scale(3) ; -ms-transform : scale(3) ; transform : scale(3) } ', 'body { background-color : #212121 ; color : #4db1bc ; margin : 0 ; display : grid ; grid-template-columns : 1fr ; grid-template-rows : 1fr ; align-items : center ; justify-items : center } ', 'body img { width : 100% ; height : 100vh ; object-fit : cover ; grid-column : 1 ; grid-row : 1 ; opacity : .7 ; z-index : 0 } ', 'img { width : 100% ; height : 100vh ; object-fit : cover ; grid-column : 1 ; grid-row : 1 ; opacity : .7 ; z-index : 0 } ', \"body h1 { margin : 0 ; padding-bottom : 6rem ; grid-column : 1 ; grid-row : 1 ; z-index : 1 ; font-family : 'Teko',sans-serif ; font-size : 10rem ; text-transform : uppercase ; animation : glow 2s ease-in-out infinite alternate ; text-align : center } \", \"h1 { margin : 0 ; padding-bottom : 6rem ; grid-column : 1 ; grid-row : 1 ; z-index : 1 ; font-family : 'Teko',sans-serif ; font-size : 10rem ; text-transform : uppercase ; animation : glow 2s ease-in-out infinite alternate ; text-align : center } \", \"body a { font-family : 'Teko',sans-serif ; color : #4db1bc ; grid-column : 1 ; grid-row : 1 ; align-self : end ; justify-self : center ; padding-bottom : 1rem } \", \"a { font-family : 'Teko',sans-serif ; color : #4db1bc ; grid-column : 1 ; grid-row : 1 ; align-self : end ; justify-self : center ; padding-bottom : 1rem } \", '@keyframes glow { from { text-shadow : 0 0 20px #2d9da9 }  to { text-shadow : 0 0 30px #34b3c1,0 0 10px #4dbbc7 }  } ', 'from { text-shadow : 0 0 20px #2d9da9 } ', 'to { text-shadow : 0 0 30px #34b3c1,0 0 10px #4dbbc7 } ', \"body { font-size : 16px ; color : #fff ; background : #C13237 ; font-family : 'Raleway',arial,sans-serif } \", 'h1 { text-align : center ; font-size : 6em ; font-weight : 300 ; position : absolute ; margin : 0 ; top : 50% ; left : 50% ; -webkit-transform : translate(-50%,-50%) ; transform : translate(-50%,-50%) } ', 'ul { padding-left : 1em ; position : fixed ; bottom : 0 ; font-weight : 400 ; z-index : 100 ; min-width : 20em } ', 'li { list-style : none ; margin : .5em 0 0 ; font-size : 1.75em } ', 'li : hover span { opacity : 1 ; transition : 0.25s } ', 'li.invisible { opacity : 0 ; visibility : hidden ; transition : all 0.25s } ', 'li.animate { opacity : 0 ; animation-duration : 0.25s ; animation-name : easeOutBounce ; animation-fill-mode : forwards } ', 'span { color : #C13237 ; padding : .35em ; margin-left : .5em ; border-radius : .2em ; font-size : .75em ; transition : .3s ; background : #fff ; opacity : 0 ; position : relative } ', \"span : before { content : '' ; width : 0 ; height : 0 ; position : absolute ; left : -.4em ; top : 50% ; margin-top : -.5em ; border-style : solid ; border-width : .5em .5em .5em 0 ; border-color : transparent #fff transparent transparent } \", 'html { --i : -1 ; --j : -1 ; --r : 0 ; height : 100% ; background : url(https : //images.unsplash.com/photo-1496481995273-1ba7de6c24fd?ixlib=rb-0.3.5&q=85&fm=jpg&crop=entropy&cs=srgb&ixid=eyJhcHBfaWQiOjE0NTg5fQ&s=63d82148afc0fe5a35f9752b4d511d82) 50%/cover #777 ; background-blend-mode : luminosity } ', 'body { display : flex ; flex-wrap : wrap ; justify-content : center ; margin : 0 auto ; padding-top : 4em ; max-width : 1000px ; font : 1em/1.5 trebuchet ms,verdana,sans-serif } ', 'header { margin : 1rem ; padding : 1em 0 ; border-radius : .5em ; min-width : 95% ; background : #fff ; text-align : center } ', 'article { overflow : hidden ; margin : 1rem ; width : 21em ; min-width : 15rem ; border-radius : 1em } ', 'h3,section { display : flex ; align-items : center ; overflow : hidden ; position : relative ; padding : .5rem } ', 'h3 { display : flex ; align-items : center ; overflow : hidden ; position : relative ; padding : .5rem } ', 'section { display : flex ; align-items : center ; overflow : hidden ; position : relative ; padding : .5rem } ', \"h3 : before { position : absolute ; z-index : -1 ; top : calc(var(--j)*1rem + (1 + var(--j))*50% - var(--r)) ; left : calc(var(--i)*1rem + (1 + var(--i))*50% - var(--r)) ; padding : var(--r) ; border-radius : 50% ; box-shadow : 0 0 0 50em ; content : '' } \", \"h3 : before,section : before { position : absolute ; z-index : -1 ; top : calc(var(--j)*1rem + (1 + var(--j))*50% - var(--r)) ; left : calc(var(--i)*1rem + (1 + var(--i))*50% - var(--r)) ; padding : var(--r) ; border-radius : 50% ; box-shadow : 0 0 0 50em ; content : '' } \", \"section : before { position : absolute ; z-index : -1 ; top : calc(var(--j)*1rem + (1 + var(--j))*50% - var(--r)) ; left : calc(var(--i)*1rem + (1 + var(--i))*50% - var(--r)) ; padding : var(--r) ; border-radius : 50% ; box-shadow : 0 0 0 50em ; content : '' } \", 'h3 { justify-content : center ; color : #fff ; font-size : 1.75em ; text-align : center ; min-height : var(--r) } ', 'section { justify-content : space-between ; min-height : calc(var(--r) - 1rem) } ', 'h3 : before { opacity : .65 } ', 'section : before { color : var(--c0) } ', 'p { margin-right : 1em ; font-size : .875em } ', 'a { display : inline-block ; color : inherit ; text-decoration : none ; text-transform : uppercase ; white-space : nowrap } ', 'body,html { background-image : linear-gradient(90deg,#1d1f20,#2f3031,#1d1f20) ; height : 100% } ', 'body { background-image : linear-gradient(90deg,#1d1f20,#2f3031,#1d1f20) ; height : 100% } ', 'html { background-image : linear-gradient(90deg,#1d1f20,#2f3031,#1d1f20) ; height : 100% } ', 'div.codepen { display : block ; position : relative ; top : 50% ; height : 16em ; width : 16em ; margin : -8em auto 0 ; border-radius : 8em ; background-color : #121212 ; cursor : pointer ; transition : color 0.8s linear,background 0.8s linear,box-shadow 0.1s linear,transform 0.1s linear } ', 'div { display : block ; position : relative ; top : 50% ; height : 16em ; width : 16em ; margin : -8em auto 0 ; border-radius : 8em ; background-color : #121212 ; cursor : pointer ; transition : color 0.8s linear,background 0.8s linear,box-shadow 0.1s linear,transform 0.1s linear } ', '.codepen { display : block ; position : relative ; top : 50% ; height : 16em ; width : 16em ; margin : -8em auto 0 ; border-radius : 8em ; background-color : #121212 ; cursor : pointer ; transition : color 0.8s linear,background 0.8s linear,box-shadow 0.1s linear,transform 0.1s linear } ', '* { box-sizing : border-box } ', \"body { background : #222 ; font-family : 'Gochi Hand',sans-serif ; color : #333 } \", 'aside.context { text-align : center ; color : #fff ; line-height : 1.7 ; font-size : 20px ; letter-spacing : .5px } ', '.context { text-align : center ; color : #fff ; line-height : 1.7 ; font-size : 20px ; letter-spacing : .5px } ', 'aside { text-align : center ; color : #fff ; line-height : 1.7 ; font-size : 20px ; letter-spacing : .5px } ', 'a { text-decoration : none ; color : #fff ; padding : 3px 0 ; border-bottom : 1px dashed } ', 'a : hover { border-bottom : 1px solid } ', 'footer { text-align : center ; margin : 4em auto ; width : 100% } ', 'footer a { text-decoration : none ; display : inline-block ; width : 45px ; height : 45px ; border-radius : 50% ; background : transparent ; border : 1px dashed #fff ; color : #fff ; margin : 5px } ', 'a { text-decoration : none ; display : inline-block ; width : 45px ; height : 45px ; border-radius : 50% ; background : transparent ; border : 1px dashed #fff ; color : #fff ; margin : 5px } ', 'body { overflow-x : hidden ; font-family : Roboto Slab , Helvetica Neue , Helvetica , Arial ,sans-serif ; } ', 'p{font-size:14px;line-height:1.75}', '.img-centered{margin:0 auto}', '.bg-light-gray{background-color:#f7f7f7}', '.bg-darkest-gray{background-color:#222}', '.btn-primary{border-color:#fff;text-transform:uppercase;font-family:Montserrat,Helvetica Neue, Helvetica,Arial,sans-serif;font-weight:700;color:#fff;background-color:#3BA9E8}', '.navbar-default{border-color:transparent;background-color:#222}', \".navbar-default.navbar-brand{font-family:'Anton',sans-serif;color:#fff}\", '.navbar-default.navbar-collapse{border-color:rgba(255,255,255,.02)}', '.navbar-default.nav li a{text-transform:uppercase;font-family:Montserrat,Helvetica Neue,Helvetica,Arial,sans-serif;font-weight:400;letter-spacing:1px;color:#fff}', '.nav li a{text-transform:uppercase;font-family:Montserrat,Helvetica Neue,Helvetica,Arial,sans-serif;font-weight:400;letter-spacing:1px;color:#fff}', '.button{display:inline-block;text-align:center;vertical-align:middle;padding:16px 24px;border:1px solid #a12727;border-radius:8px;background:#ff4a4a;background:-webkit-gradient(linear,left top,left bottom,from(#ff4a4a),to(#992727));background:-moz-linear-gradient(top,#ff4a4a,#992727);background:linear-gradient(to bottom,#ff4a4a,#992727);-webkit-box-shadow:#ff5959 0 0 0 0;-moz-box-shadow:#ff5959 0 0 0 0;box-shadow:#ff5959 0 0 0 0;text-shadow:#591717 1px 1px 1px;font:normal normal bold 22px verdana;color:#fff;text-decoration:none}', '.button{display:inline-block;text-align:center;vertical-align:middle;padding:16px 24px;border:1px solid #0e2c7a;border-radius:8px;background:#1a53e5;background:-webkit-gradient(linear,left top,left bottom,from(#1a53e5),to(#0e2c7a));background:-moz-linear-gradient(top,#1a53e5,#0e2c7a);background:linear-gradient(to bottom,#1a53e5,#0e2c7a);-webkit-box-shadow:#1f64ff 0 0 0 0;-moz-box-shadow:#1f64ff 0 0 0 0;box-shadow:#1f64ff 0 0 0 0;text-shadow:#081a48 1px 1px 1px;font:normal normal bold 22px verdana;color:#fff;text-decoration:none}', '.btn{display:inline-block;text-align:center;vertical-align:middle;padding:16px 24px;border:1px solid #0e2c7a;border-radius:8px;background:#1a53e5;background:-webkit-gradient(linear,left top,left bottom,from(#1a53e5),to(#0e2c7a));background:-moz-linear-gradient(top,#1a53e5,#0e2c7a);background:linear-gradient(to bottom,#1a53e5,#0e2c7a);-webkit-box-shadow:#1f64ff 0 0 0 0;-moz-box-shadow:#1f64ff 0 0 0 0;box-shadow:#1f64ff 0 0 0 0;text-shadow:#081a48 1px 1px 1px;font:normal normal bold 22px verdana;color:#fff;text-decoration:none}', '.btn{display:inline-block;text-align:center;vertical-align:middle;padding:16px 24px;border:1px solid #a12727;border-radius:8px;background:#ff4a4a;background:-webkit-gradient(linear,left top,left bottom,from(#ff4a4a),to(#992727));background:-moz-linear-gradient(top,#ff4a4a,#992727);background:linear-gradient(to bottom,#ff4a4a,#992727);-webkit-box-shadow:#ff5959 0 0 0 0;-moz-box-shadow:#ff5959 0 0 0 0;box-shadow:#ff5959 0 0 0 0;text-shadow:#591717 1px 1px 1px;font:normal normal bold 22px verdana;color:#fff;text-decoration:none}']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(selectors)\n",
        "print(data_css[1][0])"
      ],
      "metadata": {
        "id": "BydQ0GTGejDC",
        "outputId": "65fd0947-2cc9-4c30-b189-64fa27403f01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['body', 'div.drop-container', 'div.drop', 'h1', '@font-face', 'body', '.logo', '.switch-left', '.pad-left', '.switch-right', '.pad-right', '.line', '.text', '.text', '.text', '.reload', '.logo', 'html,body', 'body', '*', '*', '*', '.css_lab', '.witch', 'body', 'body img', 'img', 'body h1', 'h1', 'body a', 'a', '@keyframes glow', 'from', 'to', 'body', 'h1', 'ul', 'li', 'li : hover span', 'li.invisible', 'li.animate', 'span', 'span : before', 'html', 'body', 'header', 'article', 'h3,section', 'h3', 'section', 'h3 : before', 'h3 : before,section : before', 'section : before', 'h3', 'section', 'h3 : before', 'section : before', 'p', 'a', 'body,html', 'body', 'html', 'div.codepen', 'div', '.codepen', '*', 'body', 'aside.context', '.context', 'aside', 'a', 'a : hover', 'footer', 'footer a', 'a', 'body', 'p', '.img-centered', '.bg-light-gray', '.bg-darkest-gray', '.btn-primary', '.navbar-default', '.navbar-default.navbar-brand', '.navbar-default.navbar-collapse', '.navbar-default.nav li a', '.nav li a', '.button', '.button', '.button', '.button']\n",
            "body { overflow : hidden ; background : rgb(25,35,125) } \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creaemos los tokens para el manejo de textos"
      ],
      "metadata": {
        "id": "4id5FKKee76x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "selector_tokens = []\n",
        "for sentence in selectors:\n",
        "  selector_tokens.append(sentence.split(' '))\n",
        "\n",
        "print(selector_tokens[0])\n",
        "\n",
        "style_tokens = []\n",
        "for sentence in styles:\n",
        "  style_tokens.append(sentence.split(' '))\n",
        "\n",
        "print(style_tokens[89])"
      ],
      "metadata": {
        "id": "xZHB0j-Je7rR",
        "outputId": "616e7219-db2f-4d2c-c167-ce2327d6fedc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['body']\n",
            "['.btn{display:inline-block;text-align:center;vertical-align:middle;padding:16px', '24px;border:1px', 'solid', '#a12727;border-radius:8px;background:#ff4a4a;background:-webkit-gradient(linear,left', 'top,left', 'bottom,from(#ff4a4a),to(#992727));background:-moz-linear-gradient(top,#ff4a4a,#992727);background:linear-gradient(to', 'bottom,#ff4a4a,#992727);-webkit-box-shadow:#ff5959', '0', '0', '0', '0;-moz-box-shadow:#ff5959', '0', '0', '0', '0;box-shadow:#ff5959', '0', '0', '0', '0;text-shadow:#591717', '1px', '1px', '1px;font:normal', 'normal', 'bold', '22px', 'verdana;color:#fff;text-decoration:none}']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos una funcion para construir finalmente los tokens"
      ],
      "metadata": {
        "id": "hJ3RK6HlijCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_token_dict(token_list):\n",
        "  token_dict = {\n",
        "      '<PAD>': 0,\n",
        "      '<START>': 1,\n",
        "      '<END>': 2\n",
        "  }\n",
        "\n",
        "  # aqui leemos el token creado\n",
        "  for tokens in token_list:\n",
        "    for token in tokens:\n",
        "      if token not in token_dict:\n",
        "        token_dict[token] = len(token_dict)\n",
        "  \n",
        "  return token_dict"
      ],
      "metadata": {
        "id": "UX1hekMahZDU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos los tokens usando nuestra funcion anterior"
      ],
      "metadata": {
        "id": "XrsZVet8itv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# estos son los tokens que le pasaremos al encoder que seran los selectores\n",
        "selector_token_dict = build_token_dict(selector_tokens)\n",
        "\n",
        "# estos son los tokens que le pasaremos al decoder que seran los styles\n",
        "style_token_dict = build_token_dict(style_tokens)\n",
        "\n",
        "# por ultimo invertimos nuestro diccionario ya que nos tiene que devolver texto no numeros\n",
        "style_token_dict_inv = {v:k for k,v in style_token_dict.items()}\n",
        "\n",
        "# Imprimimos nuestros diccinarios\n",
        "print(selector_token_dict)\n",
        "print(style_token_dict)\n",
        "print(style_token_dict_inv)"
      ],
      "metadata": {
        "id": "Co1St5KxitGS",
        "outputId": "367d532a-5957-4efc-930b-bc23b1ab0701",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<PAD>': 0, '<START>': 1, '<END>': 2, 'body': 3, 'div.drop-container': 4, 'div.drop': 5, 'h1': 6, '@font-face': 7, '.logo': 8, '.switch-left': 9, '.pad-left': 10, '.switch-right': 11, '.pad-right': 12, '.line': 13, '.text': 14, '.reload': 15, 'html,body': 16, '*': 17, '.css_lab': 18, '.witch': 19, 'img': 20, 'a': 21, '@keyframes': 22, 'glow': 23, 'from': 24, 'to': 25, 'ul': 26, 'li': 27, ':': 28, 'hover': 29, 'span': 30, 'li.invisible': 31, 'li.animate': 32, 'before': 33, 'html': 34, 'header': 35, 'article': 36, 'h3,section': 37, 'h3': 38, 'section': 39, 'before,section': 40, 'p': 41, 'body,html': 42, 'div.codepen': 43, 'div': 44, '.codepen': 45, 'aside.context': 46, '.context': 47, 'aside': 48, 'footer': 49, '.img-centered': 50, '.bg-light-gray': 51, '.bg-darkest-gray': 52, '.btn-primary': 53, '.navbar-default': 54, '.navbar-default.navbar-brand': 55, '.navbar-default.navbar-collapse': 56, '.navbar-default.nav': 57, '.nav': 58, '.button': 59}\n",
            "{'<PAD>': 0, '<START>': 1, '<END>': 2, 'body': 3, '{': 4, 'overflow': 5, ':': 6, 'hidden': 7, ';': 8, 'background': 9, 'rgb(25,35,125)': 10, '}': 11, '': 12, 'div.drop-container': 13, 'position': 14, 'absolute': 15, 'top': 16, '0': 17, 'right': 18, 'bottom': 19, 'left': 20, 'margin': 21, 'auto': 22, 'height': 23, '200px': 24, 'width': 25, 'div.drop': 26, '-25%': 27, '100%': 28, 'border-radius': 29, '5%': 30, '-webkit-transform': 31, 'rotate(-45deg)': 32, 'transform': 33, 'deepskyblue': 34, '-webkit-animation': 35, 'drip': 36, '4s': 37, 'forwards': 38, 'animation': 39, 'h1': 40, 'color': 41, '#fff': 42, 'font-size': 43, '2.5em': 44, '1em': 45, 'z-index': 46, '2': 47, 'text-align': 48, 'center': 49, 'opacity': 50, 'appear': 51, '2s': 52, '2.5s': 53, '@font-face': 54, 'font-family': 55, 'logofont': 56, 'src': 57, 'url(https': 58, '//s3-us-west-2.amazonaws.com/s.cdpn.io/531144/couture-bld.otf)': 59, '0px': 60, '#EF5350': 61, '.logo': 62, 'scale(0.7)': 63, 'margin-top': 64, '45px': 65, '.switch-left': 66, 'fixed': 67, 'transparent': 68, '130px': 69, '380px': 70, '100px': 71, '5px': 72, 'border': 73, '35px': 74, 'solid': 75, 'translate(calc(50vw': 76, '-': 77, '217px),calc(50vh': 78, '225px))': 79, 'switch-left-animation': 80, '1.75s': 81, 'ease': 82, '.pad-left': 83, '80px': 84, 'margin-left': 85, '27.5px': 86, '.switch-right': 87, '185px': 88, '450px': 89, '+': 90, '17px),calc(50vh': 91, 'switch-right-animation': 92, '.pad-right': 93, '225px': 94, '50px': 95, '.line': 96, '6px': 97, '100vh': 98, 'calc(50vw': 99, '3px)': 100, '.25': 101, '.text': 102, 'letter-spacing': 103, '19px': 104, '288px),calc(50vh': 105, '175px))': 106, '124px': 107, '13px': 108, '290px),calc(50vh': 109, '200px))': 110, 'text-animation': 111, '.reload': 112, '75px': 113, '37.5px)': 114, 'calc(50vh': 115, '345px)': 116, 'cursor': 117, 'pointer': 118, 'transition': 119, '0.3s': 120, 'reload-animation': 121, 'scale(0.45)': 122, '65px': 123, 'html,body': 124, 'padding': 125, '*': 126, 'box-sizing': 127, 'border-box': 128, '.css_lab': 129, '#224': 130, '.witch': 131, '50%': 132, '-60px': 133, 'scale(3)': 134, '-ms-transform': 135, 'background-color': 136, '#212121': 137, '#4db1bc': 138, 'display': 139, 'grid': 140, 'grid-template-columns': 141, '1fr': 142, 'grid-template-rows': 143, 'align-items': 144, 'justify-items': 145, 'img': 146, 'object-fit': 147, 'cover': 148, 'grid-column': 149, '1': 150, 'grid-row': 151, '.7': 152, 'padding-bottom': 153, '6rem': 154, \"'Teko',sans-serif\": 155, '10rem': 156, 'text-transform': 157, 'uppercase': 158, 'glow': 159, 'ease-in-out': 160, 'infinite': 161, 'alternate': 162, 'a': 163, 'align-self': 164, 'end': 165, 'justify-self': 166, '1rem': 167, '@keyframes': 168, 'from': 169, 'text-shadow': 170, '20px': 171, '#2d9da9': 172, 'to': 173, '30px': 174, '#34b3c1,0': 175, '10px': 176, '#4dbbc7': 177, '16px': 178, '#C13237': 179, \"'Raleway',arial,sans-serif\": 180, '6em': 181, 'font-weight': 182, '300': 183, 'translate(-50%,-50%)': 184, 'ul': 185, 'padding-left': 186, '400': 187, '100': 188, 'min-width': 189, '20em': 190, 'li': 191, 'list-style': 192, 'none': 193, '.5em': 194, '1.75em': 195, 'hover': 196, 'span': 197, '0.25s': 198, 'li.invisible': 199, 'visibility': 200, 'all': 201, 'li.animate': 202, 'animation-duration': 203, 'animation-name': 204, 'easeOutBounce': 205, 'animation-fill-mode': 206, '.35em': 207, '.2em': 208, '.75em': 209, '.3s': 210, 'relative': 211, 'before': 212, 'content': 213, \"''\": 214, '-.4em': 215, '-.5em': 216, 'border-style': 217, 'border-width': 218, 'border-color': 219, 'html': 220, '--i': 221, '-1': 222, '--j': 223, '--r': 224, '//images.unsplash.com/photo-1496481995273-1ba7de6c24fd?ixlib=rb-0.3.5&q=85&fm=jpg&crop=entropy&cs=srgb&ixid=eyJhcHBfaWQiOjE0NTg5fQ&s=63d82148afc0fe5a35f9752b4d511d82)': 225, '50%/cover': 226, '#777': 227, 'background-blend-mode': 228, 'luminosity': 229, 'flex': 230, 'flex-wrap': 231, 'wrap': 232, 'justify-content': 233, 'padding-top': 234, '4em': 235, 'max-width': 236, '1000px': 237, 'font': 238, '1em/1.5': 239, 'trebuchet': 240, 'ms,verdana,sans-serif': 241, 'header': 242, '95%': 243, 'article': 244, '21em': 245, '15rem': 246, 'h3,section': 247, '.5rem': 248, 'h3': 249, 'section': 250, 'calc(var(--j)*1rem': 251, '(1': 252, 'var(--j))*50%': 253, 'var(--r))': 254, 'calc(var(--i)*1rem': 255, 'var(--i))*50%': 256, 'var(--r)': 257, 'box-shadow': 258, '50em': 259, 'before,section': 260, 'min-height': 261, 'space-between': 262, 'calc(var(--r)': 263, '1rem)': 264, '.65': 265, 'var(--c0)': 266, 'p': 267, 'margin-right': 268, '.875em': 269, 'inline-block': 270, 'inherit': 271, 'text-decoration': 272, 'white-space': 273, 'nowrap': 274, 'body,html': 275, 'background-image': 276, 'linear-gradient(90deg,#1d1f20,#2f3031,#1d1f20)': 277, 'div.codepen': 278, 'block': 279, '16em': 280, '-8em': 281, '8em': 282, '#121212': 283, '0.8s': 284, 'linear,background': 285, 'linear,box-shadow': 286, '0.1s': 287, 'linear,transform': 288, 'linear': 289, 'div': 290, '.codepen': 291, '#222': 292, \"'Gochi\": 293, \"Hand',sans-serif\": 294, '#333': 295, 'aside.context': 296, 'line-height': 297, '1.7': 298, '.5px': 299, '.context': 300, 'aside': 301, '3px': 302, 'border-bottom': 303, '1px': 304, 'dashed': 305, 'footer': 306, 'overflow-x': 307, 'Roboto': 308, 'Slab': 309, ',': 310, 'Helvetica': 311, 'Neue': 312, 'Arial': 313, ',sans-serif': 314, 'p{font-size:14px;line-height:1.75}': 315, '.img-centered{margin:0': 316, 'auto}': 317, '.bg-light-gray{background-color:#f7f7f7}': 318, '.bg-darkest-gray{background-color:#222}': 319, '.btn-primary{border-color:#fff;text-transform:uppercase;font-family:Montserrat,Helvetica': 320, 'Neue,': 321, 'Helvetica,Arial,sans-serif;font-weight:700;color:#fff;background-color:#3BA9E8}': 322, '.navbar-default{border-color:transparent;background-color:#222}': 323, \".navbar-default.navbar-brand{font-family:'Anton',sans-serif;color:#fff}\": 324, '.navbar-default.navbar-collapse{border-color:rgba(255,255,255,.02)}': 325, '.navbar-default.nav': 326, 'a{text-transform:uppercase;font-family:Montserrat,Helvetica': 327, 'Neue,Helvetica,Arial,sans-serif;font-weight:400;letter-spacing:1px;color:#fff}': 328, '.nav': 329, '.button{display:inline-block;text-align:center;vertical-align:middle;padding:16px': 330, '24px;border:1px': 331, '#a12727;border-radius:8px;background:#ff4a4a;background:-webkit-gradient(linear,left': 332, 'top,left': 333, 'bottom,from(#ff4a4a),to(#992727));background:-moz-linear-gradient(top,#ff4a4a,#992727);background:linear-gradient(to': 334, 'bottom,#ff4a4a,#992727);-webkit-box-shadow:#ff5959': 335, '0;-moz-box-shadow:#ff5959': 336, '0;box-shadow:#ff5959': 337, '0;text-shadow:#591717': 338, '1px;font:normal': 339, 'normal': 340, 'bold': 341, '22px': 342, 'verdana;color:#fff;text-decoration:none}': 343, '#0e2c7a;border-radius:8px;background:#1a53e5;background:-webkit-gradient(linear,left': 344, 'bottom,from(#1a53e5),to(#0e2c7a));background:-moz-linear-gradient(top,#1a53e5,#0e2c7a);background:linear-gradient(to': 345, 'bottom,#1a53e5,#0e2c7a);-webkit-box-shadow:#1f64ff': 346, '0;-moz-box-shadow:#1f64ff': 347, '0;box-shadow:#1f64ff': 348, '0;text-shadow:#081a48': 349, '.btn{display:inline-block;text-align:center;vertical-align:middle;padding:16px': 350}\n",
            "{0: '<PAD>', 1: '<START>', 2: '<END>', 3: 'body', 4: '{', 5: 'overflow', 6: ':', 7: 'hidden', 8: ';', 9: 'background', 10: 'rgb(25,35,125)', 11: '}', 12: '', 13: 'div.drop-container', 14: 'position', 15: 'absolute', 16: 'top', 17: '0', 18: 'right', 19: 'bottom', 20: 'left', 21: 'margin', 22: 'auto', 23: 'height', 24: '200px', 25: 'width', 26: 'div.drop', 27: '-25%', 28: '100%', 29: 'border-radius', 30: '5%', 31: '-webkit-transform', 32: 'rotate(-45deg)', 33: 'transform', 34: 'deepskyblue', 35: '-webkit-animation', 36: 'drip', 37: '4s', 38: 'forwards', 39: 'animation', 40: 'h1', 41: 'color', 42: '#fff', 43: 'font-size', 44: '2.5em', 45: '1em', 46: 'z-index', 47: '2', 48: 'text-align', 49: 'center', 50: 'opacity', 51: 'appear', 52: '2s', 53: '2.5s', 54: '@font-face', 55: 'font-family', 56: 'logofont', 57: 'src', 58: 'url(https', 59: '//s3-us-west-2.amazonaws.com/s.cdpn.io/531144/couture-bld.otf)', 60: '0px', 61: '#EF5350', 62: '.logo', 63: 'scale(0.7)', 64: 'margin-top', 65: '45px', 66: '.switch-left', 67: 'fixed', 68: 'transparent', 69: '130px', 70: '380px', 71: '100px', 72: '5px', 73: 'border', 74: '35px', 75: 'solid', 76: 'translate(calc(50vw', 77: '-', 78: '217px),calc(50vh', 79: '225px))', 80: 'switch-left-animation', 81: '1.75s', 82: 'ease', 83: '.pad-left', 84: '80px', 85: 'margin-left', 86: '27.5px', 87: '.switch-right', 88: '185px', 89: '450px', 90: '+', 91: '17px),calc(50vh', 92: 'switch-right-animation', 93: '.pad-right', 94: '225px', 95: '50px', 96: '.line', 97: '6px', 98: '100vh', 99: 'calc(50vw', 100: '3px)', 101: '.25', 102: '.text', 103: 'letter-spacing', 104: '19px', 105: '288px),calc(50vh', 106: '175px))', 107: '124px', 108: '13px', 109: '290px),calc(50vh', 110: '200px))', 111: 'text-animation', 112: '.reload', 113: '75px', 114: '37.5px)', 115: 'calc(50vh', 116: '345px)', 117: 'cursor', 118: 'pointer', 119: 'transition', 120: '0.3s', 121: 'reload-animation', 122: 'scale(0.45)', 123: '65px', 124: 'html,body', 125: 'padding', 126: '*', 127: 'box-sizing', 128: 'border-box', 129: '.css_lab', 130: '#224', 131: '.witch', 132: '50%', 133: '-60px', 134: 'scale(3)', 135: '-ms-transform', 136: 'background-color', 137: '#212121', 138: '#4db1bc', 139: 'display', 140: 'grid', 141: 'grid-template-columns', 142: '1fr', 143: 'grid-template-rows', 144: 'align-items', 145: 'justify-items', 146: 'img', 147: 'object-fit', 148: 'cover', 149: 'grid-column', 150: '1', 151: 'grid-row', 152: '.7', 153: 'padding-bottom', 154: '6rem', 155: \"'Teko',sans-serif\", 156: '10rem', 157: 'text-transform', 158: 'uppercase', 159: 'glow', 160: 'ease-in-out', 161: 'infinite', 162: 'alternate', 163: 'a', 164: 'align-self', 165: 'end', 166: 'justify-self', 167: '1rem', 168: '@keyframes', 169: 'from', 170: 'text-shadow', 171: '20px', 172: '#2d9da9', 173: 'to', 174: '30px', 175: '#34b3c1,0', 176: '10px', 177: '#4dbbc7', 178: '16px', 179: '#C13237', 180: \"'Raleway',arial,sans-serif\", 181: '6em', 182: 'font-weight', 183: '300', 184: 'translate(-50%,-50%)', 185: 'ul', 186: 'padding-left', 187: '400', 188: '100', 189: 'min-width', 190: '20em', 191: 'li', 192: 'list-style', 193: 'none', 194: '.5em', 195: '1.75em', 196: 'hover', 197: 'span', 198: '0.25s', 199: 'li.invisible', 200: 'visibility', 201: 'all', 202: 'li.animate', 203: 'animation-duration', 204: 'animation-name', 205: 'easeOutBounce', 206: 'animation-fill-mode', 207: '.35em', 208: '.2em', 209: '.75em', 210: '.3s', 211: 'relative', 212: 'before', 213: 'content', 214: \"''\", 215: '-.4em', 216: '-.5em', 217: 'border-style', 218: 'border-width', 219: 'border-color', 220: 'html', 221: '--i', 222: '-1', 223: '--j', 224: '--r', 225: '//images.unsplash.com/photo-1496481995273-1ba7de6c24fd?ixlib=rb-0.3.5&q=85&fm=jpg&crop=entropy&cs=srgb&ixid=eyJhcHBfaWQiOjE0NTg5fQ&s=63d82148afc0fe5a35f9752b4d511d82)', 226: '50%/cover', 227: '#777', 228: 'background-blend-mode', 229: 'luminosity', 230: 'flex', 231: 'flex-wrap', 232: 'wrap', 233: 'justify-content', 234: 'padding-top', 235: '4em', 236: 'max-width', 237: '1000px', 238: 'font', 239: '1em/1.5', 240: 'trebuchet', 241: 'ms,verdana,sans-serif', 242: 'header', 243: '95%', 244: 'article', 245: '21em', 246: '15rem', 247: 'h3,section', 248: '.5rem', 249: 'h3', 250: 'section', 251: 'calc(var(--j)*1rem', 252: '(1', 253: 'var(--j))*50%', 254: 'var(--r))', 255: 'calc(var(--i)*1rem', 256: 'var(--i))*50%', 257: 'var(--r)', 258: 'box-shadow', 259: '50em', 260: 'before,section', 261: 'min-height', 262: 'space-between', 263: 'calc(var(--r)', 264: '1rem)', 265: '.65', 266: 'var(--c0)', 267: 'p', 268: 'margin-right', 269: '.875em', 270: 'inline-block', 271: 'inherit', 272: 'text-decoration', 273: 'white-space', 274: 'nowrap', 275: 'body,html', 276: 'background-image', 277: 'linear-gradient(90deg,#1d1f20,#2f3031,#1d1f20)', 278: 'div.codepen', 279: 'block', 280: '16em', 281: '-8em', 282: '8em', 283: '#121212', 284: '0.8s', 285: 'linear,background', 286: 'linear,box-shadow', 287: '0.1s', 288: 'linear,transform', 289: 'linear', 290: 'div', 291: '.codepen', 292: '#222', 293: \"'Gochi\", 294: \"Hand',sans-serif\", 295: '#333', 296: 'aside.context', 297: 'line-height', 298: '1.7', 299: '.5px', 300: '.context', 301: 'aside', 302: '3px', 303: 'border-bottom', 304: '1px', 305: 'dashed', 306: 'footer', 307: 'overflow-x', 308: 'Roboto', 309: 'Slab', 310: ',', 311: 'Helvetica', 312: 'Neue', 313: 'Arial', 314: ',sans-serif', 315: 'p{font-size:14px;line-height:1.75}', 316: '.img-centered{margin:0', 317: 'auto}', 318: '.bg-light-gray{background-color:#f7f7f7}', 319: '.bg-darkest-gray{background-color:#222}', 320: '.btn-primary{border-color:#fff;text-transform:uppercase;font-family:Montserrat,Helvetica', 321: 'Neue,', 322: 'Helvetica,Arial,sans-serif;font-weight:700;color:#fff;background-color:#3BA9E8}', 323: '.navbar-default{border-color:transparent;background-color:#222}', 324: \".navbar-default.navbar-brand{font-family:'Anton',sans-serif;color:#fff}\", 325: '.navbar-default.navbar-collapse{border-color:rgba(255,255,255,.02)}', 326: '.navbar-default.nav', 327: 'a{text-transform:uppercase;font-family:Montserrat,Helvetica', 328: 'Neue,Helvetica,Arial,sans-serif;font-weight:400;letter-spacing:1px;color:#fff}', 329: '.nav', 330: '.button{display:inline-block;text-align:center;vertical-align:middle;padding:16px', 331: '24px;border:1px', 332: '#a12727;border-radius:8px;background:#ff4a4a;background:-webkit-gradient(linear,left', 333: 'top,left', 334: 'bottom,from(#ff4a4a),to(#992727));background:-moz-linear-gradient(top,#ff4a4a,#992727);background:linear-gradient(to', 335: 'bottom,#ff4a4a,#992727);-webkit-box-shadow:#ff5959', 336: '0;-moz-box-shadow:#ff5959', 337: '0;box-shadow:#ff5959', 338: '0;text-shadow:#591717', 339: '1px;font:normal', 340: 'normal', 341: 'bold', 342: '22px', 343: 'verdana;color:#fff;text-decoration:none}', 344: '#0e2c7a;border-radius:8px;background:#1a53e5;background:-webkit-gradient(linear,left', 345: 'bottom,from(#1a53e5),to(#0e2c7a));background:-moz-linear-gradient(top,#1a53e5,#0e2c7a);background:linear-gradient(to', 346: 'bottom,#1a53e5,#0e2c7a);-webkit-box-shadow:#1f64ff', 347: '0;-moz-box-shadow:#1f64ff', 348: '0;box-shadow:#1f64ff', 349: '0;text-shadow:#081a48', 350: '.btn{display:inline-block;text-align:center;vertical-align:middle;padding:16px'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agregamos cada token START , PAD y END a cada frase de nuestro datos para tener longitud igual para no causar errores de tama√±o en numpy(dado que en numpy todos lo vectores deben ser de la misma longitud)"
      ],
      "metadata": {
        "id": "gru0wBgBkT7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_tokens = [['<START>'] + tokens + ['<END>'] for tokens in selector_tokens]\n",
        "decoder_tokens = [['<START>'] + tokens + ['<END>'] for tokens in style_tokens]\n",
        "output_tokens = [ tokens + ['<END>'] for tokens in style_tokens]\n",
        "\n",
        "# Vamos a buscar el elemento con mayor longitud para darle la misma longitud a todos los elementos y evitar el error de longitud en numpy\n",
        "selector_max_len = max(map(len, encoder_tokens))\n",
        "styles_max_len = max(map(len, decoder_tokens))\n",
        "\n",
        "# aqui agregamos el token <PAD> a los valores directamente\n",
        "encoder_tokens = [ tokens + ['<PAD>'] * (selector_max_len - len(tokens)) for tokens in encoder_tokens]\n",
        "decoder_tokens = [ tokens + ['<PAD>'] * (styles_max_len - len(tokens)) for tokens in decoder_tokens]\n",
        "output_tokens = [ tokens + ['<PAD>'] * (styles_max_len - len(tokens)) for tokens in output_tokens]"
      ],
      "metadata": {
        "id": "CyUSKbctknlN"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ahora podemos ver que esta tokenizado nuestros elementos"
      ],
      "metadata": {
        "id": "B99ijIJun_Sg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(encoder_tokens[0])\n",
        "print(decoder_tokens[0])\n",
        "print(output_tokens[0])"
      ],
      "metadata": {
        "id": "wL9FqMx8oDJG",
        "outputId": "c56b7072-e09e-4f85-f668-0a3d02c68225",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<START>', 'body', '<END>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "['<START>', 'body', '{', 'overflow', ':', 'hidden', ';', 'background', ':', 'rgb(25,35,125)', '}', '', '<END>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "['body', '{', 'overflow', ':', 'hidden', ';', 'background', ':', 'rgb(25,35,125)', '}', '', '<END>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importante!!! ahora tenemos que representar numericamente los valores para crear nuestro vector de numpy tambien podemos crear tensores de Pytorch partiendo de esta logica TOMAR EN CUENTA!!!!!!"
      ],
      "metadata": {
        "id": "9JJg5voGokjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input = [list(map(lambda x: selector_token_dict[x], tokens)) for tokens in encoder_tokens]\n",
        "decoder_input = [list(map(lambda x: style_token_dict[x], tokens)) for tokens in decoder_tokens]\n",
        "output_decoded = [list(map(lambda x: [style_token_dict[x]], tokens)) for tokens in output_tokens]"
      ],
      "metadata": {
        "id": "EBy0wxLHo0sM"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ahora si lo imprimimos nos dara todos los elementos en valores numericos"
      ],
      "metadata": {
        "id": "CIdUqJJ1qA8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(encoder_input[0])\n",
        "print(decoder_input[0])\n",
        "print(output_decoded[0])"
      ],
      "metadata": {
        "id": "H6j_1ZhWqAN8",
        "outputId": "a0bfc66a-26fb-4d43-958e-d7825be16dfd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 3, 2, 0, 0, 0, 0]\n",
            "[1, 3, 4, 5, 6, 7, 8, 9, 6, 10, 11, 12, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[[3], [4], [5], [6], [7], [8], [9], [6], [10], [11], [12], [2], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ahora si traemos nuestro modelo pre entrenado"
      ],
      "metadata": {
        "id": "RnNQ3rkSp_3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model(\n",
        "    token_num = max(len(selector_token_dict), len(style_token_dict)),\n",
        "    embed_dim = 32,\n",
        "    encoder_num = 3,\n",
        "    decoder_num = 3,\n",
        "    head_num = 4,\n",
        "    hidden_dim = 128,\n",
        "    dropout_rate = 0.05,\n",
        "    use_same_embed = False\n",
        ")\n",
        "\n",
        "model.compile('adam', 'sparse_categorical_crossentropy')\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "1ZfXti1Wqqoy",
        "outputId": "c1ca1884-36a9-425b-9e29-df9104b281ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " Encoder-Input (InputLayer)     [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " Encoder-Token-Embedding (Embed  [(None, None, 32),  11232       ['Encoder-Input[0][0]']          \n",
            " dingRet)                        (351, 32)]                                                       \n",
            "                                                                                                  \n",
            " Encoder-Embedding (TrigPosEmbe  (None, None, 32)    0           ['Encoder-Token-Embedding[0][0]']\n",
            " dding)                                                                                           \n",
            "                                                                                                  \n",
            " Encoder-1-MultiHeadSelfAttenti  (None, None, 32)    4224        ['Encoder-Embedding[0][0]']      \n",
            " on (MultiHeadAttention)                                                                          \n",
            "                                                                                                  \n",
            " Encoder-1-MultiHeadSelfAttenti  (None, None, 32)    0           ['Encoder-1-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-1-MultiHeadSelfAttenti  (None, None, 32)    0           ['Encoder-Embedding[0][0]',      \n",
            " on-Add (Add)                                                     'Encoder-1-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-1-MultiHeadSelfAttenti  (None, None, 32)    64          ['Encoder-1-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-1-FeedForward (FeedFor  (None, None, 32)    8352        ['Encoder-1-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-1-FeedForward-Dropout   (None, None, 32)    0           ['Encoder-1-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-1-FeedForward-Add (Add  (None, None, 32)    0           ['Encoder-1-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-1-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-1-FeedForward-Norm (La  (None, None, 32)    64          ['Encoder-1-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-2-MultiHeadSelfAttenti  (None, None, 32)    4224        ['Encoder-1-FeedForward-Norm[0][0\n",
            " on (MultiHeadAttention)                                         ]']                              \n",
            "                                                                                                  \n",
            " Encoder-2-MultiHeadSelfAttenti  (None, None, 32)    0           ['Encoder-2-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-2-MultiHeadSelfAttenti  (None, None, 32)    0           ['Encoder-1-FeedForward-Norm[0][0\n",
            " on-Add (Add)                                                    ]',                              \n",
            "                                                                  'Encoder-2-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-2-MultiHeadSelfAttenti  (None, None, 32)    64          ['Encoder-2-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-2-FeedForward (FeedFor  (None, None, 32)    8352        ['Encoder-2-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Encoder-2-FeedForward-Dropout   (None, None, 32)    0           ['Encoder-2-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Encoder-2-FeedForward-Add (Add  (None, None, 32)    0           ['Encoder-2-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-2-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Encoder-2-FeedForward-Norm (La  (None, None, 32)    64          ['Encoder-2-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Encoder-3-MultiHeadSelfAttenti  (None, None, 32)    4224        ['Encoder-2-FeedForward-Norm[0][0\n",
            " on (MultiHeadAttention)                                         ]']                              \n",
            "                                                                                                  \n",
            " Decoder-Input (InputLayer)     [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " Encoder-3-MultiHeadSelfAttenti  (None, None, 32)    0           ['Encoder-3-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Decoder-Token-Embedding (Embed  [(None, None, 32),  11232       ['Decoder-Input[0][0]']          \n",
            " dingRet)                        (351, 32)]                                                       \n",
            "                                                                                                  \n",
            " Encoder-3-MultiHeadSelfAttenti  (None, None, 32)    0           ['Encoder-2-FeedForward-Norm[0][0\n",
            " on-Add (Add)                                                    ]',                              \n",
            "                                                                  'Encoder-3-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Decoder-Embedding (TrigPosEmbe  (None, None, 32)    0           ['Decoder-Token-Embedding[0][0]']\n",
            " dding)                                                                                           \n",
            "                                                                                                  \n",
            " Encoder-3-MultiHeadSelfAttenti  (None, None, 32)    64          ['Encoder-3-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Decoder-1-MultiHeadSelfAttenti  (None, None, 32)    4224        ['Decoder-Embedding[0][0]']      \n",
            " on (MultiHeadAttention)                                                                          \n",
            "                                                                                                  \n",
            " Encoder-3-FeedForward (FeedFor  (None, None, 32)    8352        ['Encoder-3-MultiHeadSelfAttentio\n",
            " ward)                                                           n-Norm[0][0]']                   \n",
            "                                                                                                  \n",
            " Decoder-1-MultiHeadSelfAttenti  (None, None, 32)    0           ['Decoder-1-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Encoder-3-FeedForward-Dropout   (None, None, 32)    0           ['Encoder-3-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Decoder-1-MultiHeadSelfAttenti  (None, None, 32)    0           ['Decoder-Embedding[0][0]',      \n",
            " on-Add (Add)                                                     'Decoder-1-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Encoder-3-FeedForward-Add (Add  (None, None, 32)    0           ['Encoder-3-MultiHeadSelfAttentio\n",
            " )                                                               n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-3-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Decoder-1-MultiHeadSelfAttenti  (None, None, 32)    64          ['Decoder-1-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Encoder-3-FeedForward-Norm (La  (None, None, 32)    64          ['Encoder-3-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Decoder-1-MultiHeadQueryAttent  (None, None, 32)    4224        ['Decoder-1-MultiHeadSelfAttentio\n",
            " ion (MultiHeadAttention)                                        n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-3-FeedForward-Norm[0][0\n",
            "                                                                 ]',                              \n",
            "                                                                  'Encoder-3-FeedForward-Norm[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " Decoder-1-MultiHeadQueryAttent  (None, None, 32)    0           ['Decoder-1-MultiHeadQueryAttenti\n",
            " ion-Dropout (Dropout)                                           on[0][0]']                       \n",
            "                                                                                                  \n",
            " Decoder-1-MultiHeadQueryAttent  (None, None, 32)    0           ['Decoder-1-MultiHeadSelfAttentio\n",
            " ion-Add (Add)                                                   n-Norm[0][0]',                   \n",
            "                                                                  'Decoder-1-MultiHeadQueryAttenti\n",
            "                                                                 on-Dropout[0][0]']               \n",
            "                                                                                                  \n",
            " Decoder-1-MultiHeadQueryAttent  (None, None, 32)    64          ['Decoder-1-MultiHeadQueryAttenti\n",
            " ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n",
            "                                                                                                  \n",
            " Decoder-1-FeedForward (FeedFor  (None, None, 32)    8352        ['Decoder-1-MultiHeadQueryAttenti\n",
            " ward)                                                           on-Norm[0][0]']                  \n",
            "                                                                                                  \n",
            " Decoder-1-FeedForward-Dropout   (None, None, 32)    0           ['Decoder-1-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Decoder-1-FeedForward-Add (Add  (None, None, 32)    0           ['Decoder-1-MultiHeadQueryAttenti\n",
            " )                                                               on-Norm[0][0]',                  \n",
            "                                                                  'Decoder-1-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Decoder-1-FeedForward-Norm (La  (None, None, 32)    64          ['Decoder-1-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Decoder-2-MultiHeadSelfAttenti  (None, None, 32)    4224        ['Decoder-1-FeedForward-Norm[0][0\n",
            " on (MultiHeadAttention)                                         ]']                              \n",
            "                                                                                                  \n",
            " Decoder-2-MultiHeadSelfAttenti  (None, None, 32)    0           ['Decoder-2-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Decoder-2-MultiHeadSelfAttenti  (None, None, 32)    0           ['Decoder-1-FeedForward-Norm[0][0\n",
            " on-Add (Add)                                                    ]',                              \n",
            "                                                                  'Decoder-2-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Decoder-2-MultiHeadSelfAttenti  (None, None, 32)    64          ['Decoder-2-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Decoder-2-MultiHeadQueryAttent  (None, None, 32)    4224        ['Decoder-2-MultiHeadSelfAttentio\n",
            " ion (MultiHeadAttention)                                        n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-3-FeedForward-Norm[0][0\n",
            "                                                                 ]',                              \n",
            "                                                                  'Encoder-3-FeedForward-Norm[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " Decoder-2-MultiHeadQueryAttent  (None, None, 32)    0           ['Decoder-2-MultiHeadQueryAttenti\n",
            " ion-Dropout (Dropout)                                           on[0][0]']                       \n",
            "                                                                                                  \n",
            " Decoder-2-MultiHeadQueryAttent  (None, None, 32)    0           ['Decoder-2-MultiHeadSelfAttentio\n",
            " ion-Add (Add)                                                   n-Norm[0][0]',                   \n",
            "                                                                  'Decoder-2-MultiHeadQueryAttenti\n",
            "                                                                 on-Dropout[0][0]']               \n",
            "                                                                                                  \n",
            " Decoder-2-MultiHeadQueryAttent  (None, None, 32)    64          ['Decoder-2-MultiHeadQueryAttenti\n",
            " ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n",
            "                                                                                                  \n",
            " Decoder-2-FeedForward (FeedFor  (None, None, 32)    8352        ['Decoder-2-MultiHeadQueryAttenti\n",
            " ward)                                                           on-Norm[0][0]']                  \n",
            "                                                                                                  \n",
            " Decoder-2-FeedForward-Dropout   (None, None, 32)    0           ['Decoder-2-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Decoder-2-FeedForward-Add (Add  (None, None, 32)    0           ['Decoder-2-MultiHeadQueryAttenti\n",
            " )                                                               on-Norm[0][0]',                  \n",
            "                                                                  'Decoder-2-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Decoder-2-FeedForward-Norm (La  (None, None, 32)    64          ['Decoder-2-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Decoder-3-MultiHeadSelfAttenti  (None, None, 32)    4224        ['Decoder-2-FeedForward-Norm[0][0\n",
            " on (MultiHeadAttention)                                         ]']                              \n",
            "                                                                                                  \n",
            " Decoder-3-MultiHeadSelfAttenti  (None, None, 32)    0           ['Decoder-3-MultiHeadSelfAttentio\n",
            " on-Dropout (Dropout)                                            n[0][0]']                        \n",
            "                                                                                                  \n",
            " Decoder-3-MultiHeadSelfAttenti  (None, None, 32)    0           ['Decoder-2-FeedForward-Norm[0][0\n",
            " on-Add (Add)                                                    ]',                              \n",
            "                                                                  'Decoder-3-MultiHeadSelfAttentio\n",
            "                                                                 n-Dropout[0][0]']                \n",
            "                                                                                                  \n",
            " Decoder-3-MultiHeadSelfAttenti  (None, None, 32)    64          ['Decoder-3-MultiHeadSelfAttentio\n",
            " on-Norm (LayerNormalization)                                    n-Add[0][0]']                    \n",
            "                                                                                                  \n",
            " Decoder-3-MultiHeadQueryAttent  (None, None, 32)    4224        ['Decoder-3-MultiHeadSelfAttentio\n",
            " ion (MultiHeadAttention)                                        n-Norm[0][0]',                   \n",
            "                                                                  'Encoder-3-FeedForward-Norm[0][0\n",
            "                                                                 ]',                              \n",
            "                                                                  'Encoder-3-FeedForward-Norm[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " Decoder-3-MultiHeadQueryAttent  (None, None, 32)    0           ['Decoder-3-MultiHeadQueryAttenti\n",
            " ion-Dropout (Dropout)                                           on[0][0]']                       \n",
            "                                                                                                  \n",
            " Decoder-3-MultiHeadQueryAttent  (None, None, 32)    0           ['Decoder-3-MultiHeadSelfAttentio\n",
            " ion-Add (Add)                                                   n-Norm[0][0]',                   \n",
            "                                                                  'Decoder-3-MultiHeadQueryAttenti\n",
            "                                                                 on-Dropout[0][0]']               \n",
            "                                                                                                  \n",
            " Decoder-3-MultiHeadQueryAttent  (None, None, 32)    64          ['Decoder-3-MultiHeadQueryAttenti\n",
            " ion-Norm (LayerNormalization)                                   on-Add[0][0]']                   \n",
            "                                                                                                  \n",
            " Decoder-3-FeedForward (FeedFor  (None, None, 32)    8352        ['Decoder-3-MultiHeadQueryAttenti\n",
            " ward)                                                           on-Norm[0][0]']                  \n",
            "                                                                                                  \n",
            " Decoder-3-FeedForward-Dropout   (None, None, 32)    0           ['Decoder-3-FeedForward[0][0]']  \n",
            " (Dropout)                                                                                        \n",
            "                                                                                                  \n",
            " Decoder-3-FeedForward-Add (Add  (None, None, 32)    0           ['Decoder-3-MultiHeadQueryAttenti\n",
            " )                                                               on-Norm[0][0]',                  \n",
            "                                                                  'Decoder-3-FeedForward-Dropout[0\n",
            "                                                                 ][0]']                           \n",
            "                                                                                                  \n",
            " Decoder-3-FeedForward-Norm (La  (None, None, 32)    64          ['Decoder-3-FeedForward-Add[0][0]\n",
            " yerNormalization)                                               ']                               \n",
            "                                                                                                  \n",
            " Decoder-Output (EmbeddingSim)  (None, None, 351)    351         ['Decoder-3-FeedForward-Norm[0][0\n",
            "                                                                 ]',                              \n",
            "                                                                  'Decoder-Token-Embedding[0][1]']\n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 111,903\n",
            "Trainable params: 111,903\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "realizamos el entrenamiento"
      ],
      "metadata": {
        "id": "aJa2Ac2ssWxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = [np.array(encoder_input), np.array(decoder_input)]\n",
        "y = np.array(output_decoded)\n",
        "\n",
        "model.fit(x, y, epochs=1000, batch_size = 128)"
      ],
      "metadata": {
        "id": "MPVnF60csY-1",
        "outputId": "a4d71841-aebb-430b-b938-23cca5b19f4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "1/1 [==============================] - 17s 17s/step - loss: 2.3544\n",
            "Epoch 2/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 2.3079\n",
            "Epoch 3/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 2.2798\n",
            "Epoch 4/1000\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 2.2614\n",
            "Epoch 5/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 2.2472\n",
            "Epoch 6/1000\n",
            "1/1 [==============================] - 0s 97ms/step - loss: 2.2336\n",
            "Epoch 7/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 2.2200\n",
            "Epoch 8/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 2.2077\n",
            "Epoch 9/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 2.1945\n",
            "Epoch 10/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 2.1825\n",
            "Epoch 11/1000\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 2.1712\n",
            "Epoch 12/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 2.1587\n",
            "Epoch 13/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 2.1468\n",
            "Epoch 14/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 2.1346\n",
            "Epoch 15/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 2.1226\n",
            "Epoch 16/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 2.1105\n",
            "Epoch 17/1000\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 2.0979\n",
            "Epoch 18/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 2.0859\n",
            "Epoch 19/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 2.0742\n",
            "Epoch 20/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 2.0625\n",
            "Epoch 21/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 2.0508\n",
            "Epoch 22/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 2.0392\n",
            "Epoch 23/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 2.0270\n",
            "Epoch 24/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 2.0158\n",
            "Epoch 25/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 2.0039\n",
            "Epoch 26/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 1.9923\n",
            "Epoch 27/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 1.9809\n",
            "Epoch 28/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.9693\n",
            "Epoch 29/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 1.9577\n",
            "Epoch 30/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 1.9472\n",
            "Epoch 31/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 1.9359\n",
            "Epoch 32/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 1.9250\n",
            "Epoch 33/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 1.9144\n",
            "Epoch 34/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 1.9037\n",
            "Epoch 35/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.8928\n",
            "Epoch 36/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.8827\n",
            "Epoch 37/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.8728\n",
            "Epoch 38/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.8622\n",
            "Epoch 39/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 1.8523\n",
            "Epoch 40/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 1.8426\n",
            "Epoch 41/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 1.8332\n",
            "Epoch 42/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.8238\n",
            "Epoch 43/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 1.8140\n",
            "Epoch 44/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 1.8051\n",
            "Epoch 45/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 1.7961\n",
            "Epoch 46/1000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 1.7880\n",
            "Epoch 47/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 1.7793\n",
            "Epoch 48/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.7713\n",
            "Epoch 49/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 1.7635\n",
            "Epoch 50/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 1.7554\n",
            "Epoch 51/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.7487\n",
            "Epoch 52/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.7411\n",
            "Epoch 53/1000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 1.7344\n",
            "Epoch 54/1000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 1.7276\n",
            "Epoch 55/1000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 1.7214\n",
            "Epoch 56/1000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 1.7152\n",
            "Epoch 57/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.7092\n",
            "Epoch 58/1000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 1.7034\n",
            "Epoch 59/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 1.6973\n",
            "Epoch 60/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.6924\n",
            "Epoch 61/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.6863\n",
            "Epoch 62/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 1.6810\n",
            "Epoch 63/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 1.6757\n",
            "Epoch 64/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.6711\n",
            "Epoch 65/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 1.6668\n",
            "Epoch 66/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 1.6622\n",
            "Epoch 67/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.6582\n",
            "Epoch 68/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.6534\n",
            "Epoch 69/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 1.6498\n",
            "Epoch 70/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.6457\n",
            "Epoch 71/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.6410\n",
            "Epoch 72/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.6375\n",
            "Epoch 73/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.6340\n",
            "Epoch 74/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.6306\n",
            "Epoch 75/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.6275\n",
            "Epoch 76/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.6246\n",
            "Epoch 77/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 1.6210\n",
            "Epoch 78/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 1.6178\n",
            "Epoch 79/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.6144\n",
            "Epoch 80/1000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 1.6116\n",
            "Epoch 81/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.6086\n",
            "Epoch 82/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.6058\n",
            "Epoch 83/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 1.6030\n",
            "Epoch 84/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.6013\n",
            "Epoch 85/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.5982\n",
            "Epoch 86/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 1.5957\n",
            "Epoch 87/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.5929\n",
            "Epoch 88/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 1.5905\n",
            "Epoch 89/1000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 1.5882\n",
            "Epoch 90/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 1.5855\n",
            "Epoch 91/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.5839\n",
            "Epoch 92/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 1.5814\n",
            "Epoch 93/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 1.5792\n",
            "Epoch 94/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 1.5763\n",
            "Epoch 95/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.5751\n",
            "Epoch 96/1000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 1.5722\n",
            "Epoch 97/1000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 1.5694\n",
            "Epoch 98/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 1.5677\n",
            "Epoch 99/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 1.5637\n",
            "Epoch 100/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 1.5621\n",
            "Epoch 101/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 1.5594\n",
            "Epoch 102/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 1.5560\n",
            "Epoch 103/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 1.5547\n",
            "Epoch 104/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.5514\n",
            "Epoch 105/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 1.5494\n",
            "Epoch 106/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.5459\n",
            "Epoch 107/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 1.5440\n",
            "Epoch 108/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.5398\n",
            "Epoch 109/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 1.5386\n",
            "Epoch 110/1000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 1.5361\n",
            "Epoch 111/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 1.5353\n",
            "Epoch 112/1000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 1.5333\n",
            "Epoch 113/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.5288\n",
            "Epoch 114/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.5260\n",
            "Epoch 115/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 1.5201\n",
            "Epoch 116/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 1.5191\n",
            "Epoch 117/1000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 1.5148\n",
            "Epoch 118/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.5117\n",
            "Epoch 119/1000\n",
            "1/1 [==============================] - 0s 68ms/step - loss: 1.5079\n",
            "Epoch 120/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 1.5039\n",
            "Epoch 121/1000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 1.5017\n",
            "Epoch 122/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.4972\n",
            "Epoch 123/1000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 1.4934\n",
            "Epoch 124/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 1.4893\n",
            "Epoch 125/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.4947\n",
            "Epoch 126/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 1.4867\n",
            "Epoch 127/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.4789\n",
            "Epoch 128/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 1.4762\n",
            "Epoch 129/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 1.4719\n",
            "Epoch 130/1000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 1.4604\n",
            "Epoch 131/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 1.4622\n",
            "Epoch 132/1000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 1.4540\n",
            "Epoch 133/1000\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 1.4526\n",
            "Epoch 134/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 1.4469\n",
            "Epoch 135/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.4406\n",
            "Epoch 136/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.4439\n",
            "Epoch 137/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 1.4265\n",
            "Epoch 138/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 1.4181\n",
            "Epoch 139/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.4112\n",
            "Epoch 140/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 1.4116\n",
            "Epoch 141/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 1.4011\n",
            "Epoch 142/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.3891\n",
            "Epoch 143/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 1.3873\n",
            "Epoch 144/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 1.3869\n",
            "Epoch 145/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 1.3755\n",
            "Epoch 146/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 1.3682\n",
            "Epoch 147/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.3577\n",
            "Epoch 148/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.3556\n",
            "Epoch 149/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 1.3456\n",
            "Epoch 150/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 1.3418\n",
            "Epoch 151/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 1.3342\n",
            "Epoch 152/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 1.3290\n",
            "Epoch 153/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 1.3242\n",
            "Epoch 154/1000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 1.3194\n",
            "Epoch 155/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.3147\n",
            "Epoch 156/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 1.3062\n",
            "Epoch 157/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 1.2996\n",
            "Epoch 158/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.2963\n",
            "Epoch 159/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 1.2881\n",
            "Epoch 160/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 1.2839\n",
            "Epoch 161/1000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 1.2775\n",
            "Epoch 162/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 1.2722\n",
            "Epoch 163/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 1.2652\n",
            "Epoch 164/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 1.2593\n",
            "Epoch 165/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 1.2526\n",
            "Epoch 166/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 1.2465\n",
            "Epoch 167/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.2430\n",
            "Epoch 168/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.2373\n",
            "Epoch 169/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 1.2314\n",
            "Epoch 170/1000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 1.2262\n",
            "Epoch 171/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 1.2215\n",
            "Epoch 172/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 1.2154\n",
            "Epoch 173/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 1.2076\n",
            "Epoch 174/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 1.2050\n",
            "Epoch 175/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 1.1992\n",
            "Epoch 176/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 1.1924\n",
            "Epoch 177/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.1879\n",
            "Epoch 178/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 1.1808\n",
            "Epoch 179/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 1.1759\n",
            "Epoch 180/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.1693\n",
            "Epoch 181/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 1.1629\n",
            "Epoch 182/1000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 1.1595\n",
            "Epoch 183/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 1.1525\n",
            "Epoch 184/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 1.1495\n",
            "Epoch 185/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 1.1415\n",
            "Epoch 186/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 1.1386\n",
            "Epoch 187/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 1.1313\n",
            "Epoch 188/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 1.1267\n",
            "Epoch 189/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 1.1230\n",
            "Epoch 190/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 1.1145\n",
            "Epoch 191/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 1.1151\n",
            "Epoch 192/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 1.1088\n",
            "Epoch 193/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 1.1028\n",
            "Epoch 194/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.0970\n",
            "Epoch 195/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.0947\n",
            "Epoch 196/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.0868\n",
            "Epoch 197/1000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 1.0853\n",
            "Epoch 198/1000\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 1.0802\n",
            "Epoch 199/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 1.0656\n",
            "Epoch 200/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 1.0671\n",
            "Epoch 201/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 1.0606\n",
            "Epoch 202/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 1.0607\n",
            "Epoch 203/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 1.0493\n",
            "Epoch 204/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 1.0489\n",
            "Epoch 205/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 1.0435\n",
            "Epoch 206/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 1.0394\n",
            "Epoch 207/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 1.0290\n",
            "Epoch 208/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 1.0288\n",
            "Epoch 209/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.0203\n",
            "Epoch 210/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 1.0214\n",
            "Epoch 211/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 1.0119\n",
            "Epoch 212/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 1.0068\n",
            "Epoch 213/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 1.0063\n",
            "Epoch 214/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 1.0014\n",
            "Epoch 215/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.9981\n",
            "Epoch 216/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.9941\n",
            "Epoch 217/1000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.9853\n",
            "Epoch 218/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.9830\n",
            "Epoch 219/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.9734\n",
            "Epoch 220/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.9724\n",
            "Epoch 221/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.9654\n",
            "Epoch 222/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.9603\n",
            "Epoch 223/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.9549\n",
            "Epoch 224/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.9519\n",
            "Epoch 225/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.9434\n",
            "Epoch 226/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.9456\n",
            "Epoch 227/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.9356\n",
            "Epoch 228/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.9339\n",
            "Epoch 229/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.9284\n",
            "Epoch 230/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.9266\n",
            "Epoch 231/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.9159\n",
            "Epoch 232/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.9138\n",
            "Epoch 233/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.9115\n",
            "Epoch 234/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.9065\n",
            "Epoch 235/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.9032\n",
            "Epoch 236/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.8964\n",
            "Epoch 237/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.8981\n",
            "Epoch 238/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.8959\n",
            "Epoch 239/1000\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.8820\n",
            "Epoch 240/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.8835\n",
            "Epoch 241/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.8729\n",
            "Epoch 242/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.8790\n",
            "Epoch 243/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.8671\n",
            "Epoch 244/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.8631\n",
            "Epoch 245/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.8573\n",
            "Epoch 246/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.8533\n",
            "Epoch 247/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.8448\n",
            "Epoch 248/1000\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.8503\n",
            "Epoch 249/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.8404\n",
            "Epoch 250/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.8393\n",
            "Epoch 251/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.8313\n",
            "Epoch 252/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.8248\n",
            "Epoch 253/1000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.8251\n",
            "Epoch 254/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.8154\n",
            "Epoch 255/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.8142\n",
            "Epoch 256/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.8013\n",
            "Epoch 257/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.8021\n",
            "Epoch 258/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.7941\n",
            "Epoch 259/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.7905\n",
            "Epoch 260/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.7907\n",
            "Epoch 261/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.7798\n",
            "Epoch 262/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.7788\n",
            "Epoch 263/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.7731\n",
            "Epoch 264/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.7705\n",
            "Epoch 265/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.7644\n",
            "Epoch 266/1000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.7562\n",
            "Epoch 267/1000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.7546\n",
            "Epoch 268/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.7475\n",
            "Epoch 269/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.7461\n",
            "Epoch 270/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.7425\n",
            "Epoch 271/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.7366\n",
            "Epoch 272/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.7315\n",
            "Epoch 273/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.7309\n",
            "Epoch 274/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.7214\n",
            "Epoch 275/1000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.7218\n",
            "Epoch 276/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.7136\n",
            "Epoch 277/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.7111\n",
            "Epoch 278/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.7076\n",
            "Epoch 279/1000\n",
            "1/1 [==============================] - 0s 69ms/step - loss: 0.7044\n",
            "Epoch 280/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.6948\n",
            "Epoch 281/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.6917\n",
            "Epoch 282/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.6943\n",
            "Epoch 283/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.6874\n",
            "Epoch 284/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.6808\n",
            "Epoch 285/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.6757\n",
            "Epoch 286/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.6750\n",
            "Epoch 287/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.6717\n",
            "Epoch 288/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.6685\n",
            "Epoch 289/1000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.6614\n",
            "Epoch 290/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.6631\n",
            "Epoch 291/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.6516\n",
            "Epoch 292/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.6527\n",
            "Epoch 293/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.6459\n",
            "Epoch 294/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.6468\n",
            "Epoch 295/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.6423\n",
            "Epoch 296/1000\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.6367\n",
            "Epoch 297/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.6316\n",
            "Epoch 298/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.6256\n",
            "Epoch 299/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.6256\n",
            "Epoch 300/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.6228\n",
            "Epoch 301/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.6233\n",
            "Epoch 302/1000\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.6126\n",
            "Epoch 303/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.6134\n",
            "Epoch 304/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.6104\n",
            "Epoch 305/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.6064\n",
            "Epoch 306/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.6007\n",
            "Epoch 307/1000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.6011\n",
            "Epoch 308/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.5896\n",
            "Epoch 309/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.5925\n",
            "Epoch 310/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.5818\n",
            "Epoch 311/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.5834\n",
            "Epoch 312/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.5784\n",
            "Epoch 313/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.5743\n",
            "Epoch 314/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.5732\n",
            "Epoch 315/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.5697\n",
            "Epoch 316/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.5672\n",
            "Epoch 317/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.5616\n",
            "Epoch 318/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.5607\n",
            "Epoch 319/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.5545\n",
            "Epoch 320/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.5563\n",
            "Epoch 321/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.5492\n",
            "Epoch 322/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.5447\n",
            "Epoch 323/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.5437\n",
            "Epoch 324/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.5414\n",
            "Epoch 325/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.5367\n",
            "Epoch 326/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.5367\n",
            "Epoch 327/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.5290\n",
            "Epoch 328/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.5273\n",
            "Epoch 329/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.5181\n",
            "Epoch 330/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.5247\n",
            "Epoch 331/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.5165\n",
            "Epoch 332/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.5192\n",
            "Epoch 333/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.5150\n",
            "Epoch 334/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.5144\n",
            "Epoch 335/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.5114\n",
            "Epoch 336/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.5078\n",
            "Epoch 337/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.4995\n",
            "Epoch 338/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.4976\n",
            "Epoch 339/1000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.4982\n",
            "Epoch 340/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.4918\n",
            "Epoch 341/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.4910\n",
            "Epoch 342/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.4900\n",
            "Epoch 343/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.4823\n",
            "Epoch 344/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.4838\n",
            "Epoch 345/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.4743\n",
            "Epoch 346/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.4742\n",
            "Epoch 347/1000\n",
            "1/1 [==============================] - 0s 67ms/step - loss: 0.4727\n",
            "Epoch 348/1000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.4663\n",
            "Epoch 349/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.4697\n",
            "Epoch 350/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.4611\n",
            "Epoch 351/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.4596\n",
            "Epoch 352/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.4614\n",
            "Epoch 353/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.4613\n",
            "Epoch 354/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.4522\n",
            "Epoch 355/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.4486\n",
            "Epoch 356/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.4441\n",
            "Epoch 357/1000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.4473\n",
            "Epoch 358/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.4419\n",
            "Epoch 359/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.4360\n",
            "Epoch 360/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.4369\n",
            "Epoch 361/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.4351\n",
            "Epoch 362/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.4294\n",
            "Epoch 363/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.4286\n",
            "Epoch 364/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.4260\n",
            "Epoch 365/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.4228\n",
            "Epoch 366/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.4199\n",
            "Epoch 367/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.4144\n",
            "Epoch 368/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.4151\n",
            "Epoch 369/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.4092\n",
            "Epoch 370/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.4099\n",
            "Epoch 371/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.4058\n",
            "Epoch 372/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.4027\n",
            "Epoch 373/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.4018\n",
            "Epoch 374/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.4058\n",
            "Epoch 375/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.3946\n",
            "Epoch 376/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.3914\n",
            "Epoch 377/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.3902\n",
            "Epoch 378/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.3901\n",
            "Epoch 379/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.3839\n",
            "Epoch 380/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.3844\n",
            "Epoch 381/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.3789\n",
            "Epoch 382/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.3795\n",
            "Epoch 383/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.3722\n",
            "Epoch 384/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.3730\n",
            "Epoch 385/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.3681\n",
            "Epoch 386/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.3703\n",
            "Epoch 387/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.3669\n",
            "Epoch 388/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.3638\n",
            "Epoch 389/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.3561\n",
            "Epoch 390/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.3579\n",
            "Epoch 391/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.3585\n",
            "Epoch 392/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.3548\n",
            "Epoch 393/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.3515\n",
            "Epoch 394/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.3486\n",
            "Epoch 395/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.3452\n",
            "Epoch 396/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.3443\n",
            "Epoch 397/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.3417\n",
            "Epoch 398/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.3369\n",
            "Epoch 399/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.3370\n",
            "Epoch 400/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.3333\n",
            "Epoch 401/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.3331\n",
            "Epoch 402/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.3283\n",
            "Epoch 403/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.3235\n",
            "Epoch 404/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.3202\n",
            "Epoch 405/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.3233\n",
            "Epoch 406/1000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.3219\n",
            "Epoch 407/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.3226\n",
            "Epoch 408/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.3190\n",
            "Epoch 409/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.3162\n",
            "Epoch 410/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.3136\n",
            "Epoch 411/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.3122\n",
            "Epoch 412/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.3105\n",
            "Epoch 413/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.3054\n",
            "Epoch 414/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.3048\n",
            "Epoch 415/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.2992\n",
            "Epoch 416/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.3003\n",
            "Epoch 417/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.2999\n",
            "Epoch 418/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.2965\n",
            "Epoch 419/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.2960\n",
            "Epoch 420/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.2949\n",
            "Epoch 421/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.2912\n",
            "Epoch 422/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.2886\n",
            "Epoch 423/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.2900\n",
            "Epoch 424/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.2795\n",
            "Epoch 425/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.2854\n",
            "Epoch 426/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.2783\n",
            "Epoch 427/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.2796\n",
            "Epoch 428/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.2798\n",
            "Epoch 429/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.2760\n",
            "Epoch 430/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.2797\n",
            "Epoch 431/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.2733\n",
            "Epoch 432/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.2705\n",
            "Epoch 433/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.2696\n",
            "Epoch 434/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.2708\n",
            "Epoch 435/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.2704\n",
            "Epoch 436/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.2649\n",
            "Epoch 437/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.2650\n",
            "Epoch 438/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.2622\n",
            "Epoch 439/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.2618\n",
            "Epoch 440/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.2601\n",
            "Epoch 441/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.2578\n",
            "Epoch 442/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.2563\n",
            "Epoch 443/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.2506\n",
            "Epoch 444/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.2510\n",
            "Epoch 445/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.2493\n",
            "Epoch 446/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.2489\n",
            "Epoch 447/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.2484\n",
            "Epoch 448/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.2474\n",
            "Epoch 449/1000\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.2463\n",
            "Epoch 450/1000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.2427\n",
            "Epoch 451/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.2432\n",
            "Epoch 452/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.2380\n",
            "Epoch 453/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.2380\n",
            "Epoch 454/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.2339\n",
            "Epoch 455/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.2335\n",
            "Epoch 456/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.2283\n",
            "Epoch 457/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.2309\n",
            "Epoch 458/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.2272\n",
            "Epoch 459/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.2286\n",
            "Epoch 460/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.2238\n",
            "Epoch 461/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.2236\n",
            "Epoch 462/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.2246\n",
            "Epoch 463/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.2217\n",
            "Epoch 464/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.2221\n",
            "Epoch 465/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.2200\n",
            "Epoch 466/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.2175\n",
            "Epoch 467/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.2157\n",
            "Epoch 468/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.2179\n",
            "Epoch 469/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.2123\n",
            "Epoch 470/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.2102\n",
            "Epoch 471/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.2066\n",
            "Epoch 472/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.2049\n",
            "Epoch 473/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.2067\n",
            "Epoch 474/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.2040\n",
            "Epoch 475/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.2054\n",
            "Epoch 476/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.2037\n",
            "Epoch 477/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.2005\n",
            "Epoch 478/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.1997\n",
            "Epoch 479/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.2010\n",
            "Epoch 480/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.1986\n",
            "Epoch 481/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.2003\n",
            "Epoch 482/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.1956\n",
            "Epoch 483/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.1922\n",
            "Epoch 484/1000\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.1938\n",
            "Epoch 485/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.1941\n",
            "Epoch 486/1000\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.1907\n",
            "Epoch 487/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.1892\n",
            "Epoch 488/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.1862\n",
            "Epoch 489/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.1868\n",
            "Epoch 490/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.1860\n",
            "Epoch 491/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.1874\n",
            "Epoch 492/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.1807\n",
            "Epoch 493/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.1850\n",
            "Epoch 494/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.1796\n",
            "Epoch 495/1000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.1797\n",
            "Epoch 496/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.1811\n",
            "Epoch 497/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.1788\n",
            "Epoch 498/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.1770\n",
            "Epoch 499/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.1774\n",
            "Epoch 500/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.1741\n",
            "Epoch 501/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.1725\n",
            "Epoch 502/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.1760\n",
            "Epoch 503/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.1720\n",
            "Epoch 504/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.1720\n",
            "Epoch 505/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.1694\n",
            "Epoch 506/1000\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.1712\n",
            "Epoch 507/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.1660\n",
            "Epoch 508/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.1649\n",
            "Epoch 509/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.1623\n",
            "Epoch 510/1000\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.1653\n",
            "Epoch 511/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.1621\n",
            "Epoch 512/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.1589\n",
            "Epoch 513/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.1615\n",
            "Epoch 514/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.1640\n",
            "Epoch 515/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.1572\n",
            "Epoch 516/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.1588\n",
            "Epoch 517/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.1573\n",
            "Epoch 518/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.1563\n",
            "Epoch 519/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.1527\n",
            "Epoch 520/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.1533\n",
            "Epoch 521/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.1515\n",
            "Epoch 522/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.1540\n",
            "Epoch 523/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.1521\n",
            "Epoch 524/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.1481\n",
            "Epoch 525/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.1501\n",
            "Epoch 526/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.1483\n",
            "Epoch 527/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.1470\n",
            "Epoch 528/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.1445\n",
            "Epoch 529/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.1464\n",
            "Epoch 530/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.1428\n",
            "Epoch 531/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.1439\n",
            "Epoch 532/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.1459\n",
            "Epoch 533/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.1445\n",
            "Epoch 534/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.1412\n",
            "Epoch 535/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.1423\n",
            "Epoch 536/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.1377\n",
            "Epoch 537/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.1427\n",
            "Epoch 538/1000\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.1369\n",
            "Epoch 539/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.1388\n",
            "Epoch 540/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.1379\n",
            "Epoch 541/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.1382\n",
            "Epoch 542/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.1330\n",
            "Epoch 543/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.1331\n",
            "Epoch 544/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.1364\n",
            "Epoch 545/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.1318\n",
            "Epoch 546/1000\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 0.1329\n",
            "Epoch 547/1000\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.1317\n",
            "Epoch 548/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.1314\n",
            "Epoch 549/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.1286\n",
            "Epoch 550/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.1255\n",
            "Epoch 551/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.1296\n",
            "Epoch 552/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.1253\n",
            "Epoch 553/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.1274\n",
            "Epoch 554/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.1300\n",
            "Epoch 555/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.1241\n",
            "Epoch 556/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.1262\n",
            "Epoch 557/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.1249\n",
            "Epoch 558/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.1246\n",
            "Epoch 559/1000\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.1218\n",
            "Epoch 560/1000\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.1227\n",
            "Epoch 561/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.1223\n",
            "Epoch 562/1000\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.1208\n",
            "Epoch 563/1000\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.1184\n",
            "Epoch 564/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.1175\n",
            "Epoch 565/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.1170\n",
            "Epoch 566/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.1189\n",
            "Epoch 567/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.1138\n",
            "Epoch 568/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.1170\n",
            "Epoch 569/1000\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.1155\n",
            "Epoch 570/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.1134\n",
            "Epoch 571/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.1174\n",
            "Epoch 572/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.1124\n",
            "Epoch 573/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.1130\n",
            "Epoch 574/1000\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.1125\n",
            "Epoch 575/1000\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.1145\n",
            "Epoch 576/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.1116\n",
            "Epoch 577/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.1112\n",
            "Epoch 578/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.1073\n",
            "Epoch 579/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.1104\n",
            "Epoch 580/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.1096\n",
            "Epoch 581/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.1098\n",
            "Epoch 582/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.1085\n",
            "Epoch 583/1000\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.1120\n",
            "Epoch 584/1000\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.1062\n",
            "Epoch 585/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.1066\n",
            "Epoch 586/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.1061\n",
            "Epoch 587/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.1069\n",
            "Epoch 588/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.1058\n",
            "Epoch 589/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.1038\n",
            "Epoch 590/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.1020\n",
            "Epoch 591/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.1042\n",
            "Epoch 592/1000\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.1037\n",
            "Epoch 593/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.1021\n",
            "Epoch 594/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.1017\n",
            "Epoch 595/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.1003\n",
            "Epoch 596/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0994\n",
            "Epoch 597/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.1004\n",
            "Epoch 598/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0993\n",
            "Epoch 599/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0993\n",
            "Epoch 600/1000\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.0991\n",
            "Epoch 601/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0963\n",
            "Epoch 602/1000\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.0966\n",
            "Epoch 603/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0958\n",
            "Epoch 604/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0940\n",
            "Epoch 605/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0940\n",
            "Epoch 606/1000\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 0.0940\n",
            "Epoch 607/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0944\n",
            "Epoch 608/1000\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.0918\n",
            "Epoch 609/1000\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.0937\n",
            "Epoch 610/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0937\n",
            "Epoch 611/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0914\n",
            "Epoch 612/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0919\n",
            "Epoch 613/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0937\n",
            "Epoch 614/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0886\n",
            "Epoch 615/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0931\n",
            "Epoch 616/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0905\n",
            "Epoch 617/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0882\n",
            "Epoch 618/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0924\n",
            "Epoch 619/1000\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.0889\n",
            "Epoch 620/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0887\n",
            "Epoch 621/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0859\n",
            "Epoch 622/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0878\n",
            "Epoch 623/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0868\n",
            "Epoch 624/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0858\n",
            "Epoch 625/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0857\n",
            "Epoch 626/1000\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.0876\n",
            "Epoch 627/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0865\n",
            "Epoch 628/1000\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.0873\n",
            "Epoch 629/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0835\n",
            "Epoch 630/1000\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.0865\n",
            "Epoch 631/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0813\n",
            "Epoch 632/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0844\n",
            "Epoch 633/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0822\n",
            "Epoch 634/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0825\n",
            "Epoch 635/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0832\n",
            "Epoch 636/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0833\n",
            "Epoch 637/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0835\n",
            "Epoch 638/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0814\n",
            "Epoch 639/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0824\n",
            "Epoch 640/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0803\n",
            "Epoch 641/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0782\n",
            "Epoch 642/1000\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.0772\n",
            "Epoch 643/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0788\n",
            "Epoch 644/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0796\n",
            "Epoch 645/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0766\n",
            "Epoch 646/1000\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.0802\n",
            "Epoch 647/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0777\n",
            "Epoch 648/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0770\n",
            "Epoch 649/1000\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.0763\n",
            "Epoch 650/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0779\n",
            "Epoch 651/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0761\n",
            "Epoch 652/1000\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.0749\n",
            "Epoch 653/1000\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.0737\n",
            "Epoch 654/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0757\n",
            "Epoch 655/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0747\n",
            "Epoch 656/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0740\n",
            "Epoch 657/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0760\n",
            "Epoch 658/1000\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 0.0721\n",
            "Epoch 659/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0716\n",
            "Epoch 660/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0742\n",
            "Epoch 661/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0718\n",
            "Epoch 662/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0733\n",
            "Epoch 663/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0715\n",
            "Epoch 664/1000\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.0701\n",
            "Epoch 665/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0701\n",
            "Epoch 666/1000\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.0721\n",
            "Epoch 667/1000\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.0692\n",
            "Epoch 668/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0703\n",
            "Epoch 669/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0698\n",
            "Epoch 670/1000\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.0696\n",
            "Epoch 671/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0715\n",
            "Epoch 672/1000\n",
            "1/1 [==============================] - 0s 74ms/step - loss: 0.0657\n",
            "Epoch 673/1000\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.0686\n",
            "Epoch 674/1000\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.0670\n",
            "Epoch 675/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0682\n",
            "Epoch 676/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0689\n",
            "Epoch 677/1000\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.0681\n",
            "Epoch 678/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0692\n",
            "Epoch 679/1000\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 0.0671\n",
            "Epoch 680/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0664\n",
            "Epoch 681/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0665\n",
            "Epoch 682/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0665\n",
            "Epoch 683/1000\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.0643\n",
            "Epoch 684/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0677\n",
            "Epoch 685/1000\n",
            "1/1 [==============================] - 0s 94ms/step - loss: 0.0653\n",
            "Epoch 686/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0653\n",
            "Epoch 687/1000\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.0663\n",
            "Epoch 688/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0651\n",
            "Epoch 689/1000\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.0650\n",
            "Epoch 690/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0657\n",
            "Epoch 691/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0633\n",
            "Epoch 692/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0629\n",
            "Epoch 693/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0652\n",
            "Epoch 694/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0645\n",
            "Epoch 695/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0624\n",
            "Epoch 696/1000\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.0649\n",
            "Epoch 697/1000\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.0627\n",
            "Epoch 698/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0638\n",
            "Epoch 699/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0620\n",
            "Epoch 700/1000\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.0611\n",
            "Epoch 701/1000\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.0613\n",
            "Epoch 702/1000\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.0602\n",
            "Epoch 703/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0603\n",
            "Epoch 704/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0606\n",
            "Epoch 705/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0596\n",
            "Epoch 706/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0573\n",
            "Epoch 707/1000\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.0605\n",
            "Epoch 708/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0622\n",
            "Epoch 709/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0567\n",
            "Epoch 710/1000\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.0597\n",
            "Epoch 711/1000\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.0599\n",
            "Epoch 712/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0578\n",
            "Epoch 713/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0571\n",
            "Epoch 714/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0576\n",
            "Epoch 715/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0599\n",
            "Epoch 716/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0578\n",
            "Epoch 717/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0588\n",
            "Epoch 718/1000\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.0571\n",
            "Epoch 719/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0541\n",
            "Epoch 720/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0582\n",
            "Epoch 721/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0567\n",
            "Epoch 722/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0567\n",
            "Epoch 723/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0562\n",
            "Epoch 724/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0591\n",
            "Epoch 725/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0572\n",
            "Epoch 726/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0567\n",
            "Epoch 727/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0540\n",
            "Epoch 728/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0553\n",
            "Epoch 729/1000\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.0554\n",
            "Epoch 730/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0536\n",
            "Epoch 731/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0546\n",
            "Epoch 732/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0534\n",
            "Epoch 733/1000\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.0522\n",
            "Epoch 734/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0534\n",
            "Epoch 735/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0528\n",
            "Epoch 736/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0550\n",
            "Epoch 737/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0542\n",
            "Epoch 738/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0522\n",
            "Epoch 739/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0525\n",
            "Epoch 740/1000\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.0504\n",
            "Epoch 741/1000\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.0516\n",
            "Epoch 742/1000\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.0530\n",
            "Epoch 743/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0510\n",
            "Epoch 744/1000\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.0529\n",
            "Epoch 745/1000\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.0532\n",
            "Epoch 746/1000\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.0525\n",
            "Epoch 747/1000\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.0506\n",
            "Epoch 748/1000\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.0490\n",
            "Epoch 749/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0516\n",
            "Epoch 750/1000\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.0508\n",
            "Epoch 751/1000\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.0504\n",
            "Epoch 752/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0495\n",
            "Epoch 753/1000\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.0489\n",
            "Epoch 754/1000\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.0480\n",
            "Epoch 755/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0504\n",
            "Epoch 756/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0484\n",
            "Epoch 757/1000\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.0494\n",
            "Epoch 758/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0483\n",
            "Epoch 759/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0487\n",
            "Epoch 760/1000\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.0499\n",
            "Epoch 761/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0468\n",
            "Epoch 762/1000\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.0497\n",
            "Epoch 763/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0475\n",
            "Epoch 764/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0471\n",
            "Epoch 765/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0482\n",
            "Epoch 766/1000\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.0488\n",
            "Epoch 767/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0473\n",
            "Epoch 768/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0473\n",
            "Epoch 769/1000\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.0462\n",
            "Epoch 770/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0459\n",
            "Epoch 771/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0459\n",
            "Epoch 772/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0455\n",
            "Epoch 773/1000\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.0460\n",
            "Epoch 774/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0442\n",
            "Epoch 775/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0464\n",
            "Epoch 776/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0457\n",
            "Epoch 777/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0466\n",
            "Epoch 778/1000\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.0458\n",
            "Epoch 779/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0455\n",
            "Epoch 780/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0454\n",
            "Epoch 781/1000\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.0458\n",
            "Epoch 782/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0461\n",
            "Epoch 783/1000\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.0437\n",
            "Epoch 784/1000\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.0448\n",
            "Epoch 785/1000\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 0.0457\n",
            "Epoch 786/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0424\n",
            "Epoch 787/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0447\n",
            "Epoch 788/1000\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.0463\n",
            "Epoch 789/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0448\n",
            "Epoch 790/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0454\n",
            "Epoch 791/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0435\n",
            "Epoch 792/1000\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.0446\n",
            "Epoch 793/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0413\n",
            "Epoch 794/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0430\n",
            "Epoch 795/1000\n",
            "1/1 [==============================] - 0s 95ms/step - loss: 0.0441\n",
            "Epoch 796/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0437\n",
            "Epoch 797/1000\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.0432\n",
            "Epoch 798/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0435\n",
            "Epoch 799/1000\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.0438\n",
            "Epoch 800/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0452\n",
            "Epoch 801/1000\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.0422\n",
            "Epoch 802/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0409\n",
            "Epoch 803/1000\n",
            "1/1 [==============================] - 0s 96ms/step - loss: 0.0425\n",
            "Epoch 804/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0410\n",
            "Epoch 805/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0424\n",
            "Epoch 806/1000\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.0444\n",
            "Epoch 807/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0428\n",
            "Epoch 808/1000\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.0420\n",
            "Epoch 809/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0407\n",
            "Epoch 810/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0419\n",
            "Epoch 811/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0439\n",
            "Epoch 812/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0420\n",
            "Epoch 813/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0426\n",
            "Epoch 814/1000\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.0409\n",
            "Epoch 815/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0397\n",
            "Epoch 816/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0406\n",
            "Epoch 817/1000\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.0405\n",
            "Epoch 818/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0401\n",
            "Epoch 819/1000\n",
            "1/1 [==============================] - 0s 94ms/step - loss: 0.0423\n",
            "Epoch 820/1000\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.0397\n",
            "Epoch 821/1000\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.0418\n",
            "Epoch 822/1000\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.0393\n",
            "Epoch 823/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0383\n",
            "Epoch 824/1000\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.0412\n",
            "Epoch 825/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0416\n",
            "Epoch 826/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0370\n",
            "Epoch 827/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0382\n",
            "Epoch 828/1000\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.0388\n",
            "Epoch 829/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0405\n",
            "Epoch 830/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0400\n",
            "Epoch 831/1000\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.0397\n",
            "Epoch 832/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0392\n",
            "Epoch 833/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0394\n",
            "Epoch 834/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0383\n",
            "Epoch 835/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0385\n",
            "Epoch 836/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0386\n",
            "Epoch 837/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0386\n",
            "Epoch 838/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0368\n",
            "Epoch 839/1000\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.0382\n",
            "Epoch 840/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0363\n",
            "Epoch 841/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0378\n",
            "Epoch 842/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0391\n",
            "Epoch 843/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0378\n",
            "Epoch 844/1000\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.0380\n",
            "Epoch 845/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0368\n",
            "Epoch 846/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0376\n",
            "Epoch 847/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0360\n",
            "Epoch 848/1000\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.0380\n",
            "Epoch 849/1000\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.0362\n",
            "Epoch 850/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0371\n",
            "Epoch 851/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0379\n",
            "Epoch 852/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0392\n",
            "Epoch 853/1000\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.0381\n",
            "Epoch 854/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0361\n",
            "Epoch 855/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0342\n",
            "Epoch 856/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0365\n",
            "Epoch 857/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0381\n",
            "Epoch 858/1000\n",
            "1/1 [==============================] - 0s 93ms/step - loss: 0.0371\n",
            "Epoch 859/1000\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.0368\n",
            "Epoch 860/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0375\n",
            "Epoch 861/1000\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.0360\n",
            "Epoch 862/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0350\n",
            "Epoch 863/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0361\n",
            "Epoch 864/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0355\n",
            "Epoch 865/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0347\n",
            "Epoch 866/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0346\n",
            "Epoch 867/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0363\n",
            "Epoch 868/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0356\n",
            "Epoch 869/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0337\n",
            "Epoch 870/1000\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.0342\n",
            "Epoch 871/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0355\n",
            "Epoch 872/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0344\n",
            "Epoch 873/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0349\n",
            "Epoch 874/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.0349\n",
            "Epoch 875/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0342\n",
            "Epoch 876/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0347\n",
            "Epoch 877/1000\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.0352\n",
            "Epoch 878/1000\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.0343\n",
            "Epoch 879/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0350\n",
            "Epoch 880/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0337\n",
            "Epoch 881/1000\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.0342\n",
            "Epoch 882/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0335\n",
            "Epoch 883/1000\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.0352\n",
            "Epoch 884/1000\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.0334\n",
            "Epoch 885/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0348\n",
            "Epoch 886/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0334\n",
            "Epoch 887/1000\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.0344\n",
            "Epoch 888/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0327\n",
            "Epoch 889/1000\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.0339\n",
            "Epoch 890/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0353\n",
            "Epoch 891/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0317\n",
            "Epoch 892/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0331\n",
            "Epoch 893/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0338\n",
            "Epoch 894/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0323\n",
            "Epoch 895/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0335\n",
            "Epoch 896/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0318\n",
            "Epoch 897/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0319\n",
            "Epoch 898/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0340\n",
            "Epoch 899/1000\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.0316\n",
            "Epoch 900/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0337\n",
            "Epoch 901/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0323\n",
            "Epoch 902/1000\n",
            "1/1 [==============================] - 0s 94ms/step - loss: 0.0324\n",
            "Epoch 903/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0310\n",
            "Epoch 904/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0341\n",
            "Epoch 905/1000\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.0338\n",
            "Epoch 906/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0325\n",
            "Epoch 907/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0314\n",
            "Epoch 908/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0318\n",
            "Epoch 909/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0314\n",
            "Epoch 910/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0311\n",
            "Epoch 911/1000\n",
            "1/1 [==============================] - 0s 94ms/step - loss: 0.0314\n",
            "Epoch 912/1000\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.0330\n",
            "Epoch 913/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0325\n",
            "Epoch 914/1000\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.0304\n",
            "Epoch 915/1000\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.0327\n",
            "Epoch 916/1000\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.0325\n",
            "Epoch 917/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0310\n",
            "Epoch 918/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0324\n",
            "Epoch 919/1000\n",
            "1/1 [==============================] - 0s 75ms/step - loss: 0.0311\n",
            "Epoch 920/1000\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.0298\n",
            "Epoch 921/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0317\n",
            "Epoch 922/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0311\n",
            "Epoch 923/1000\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.0307\n",
            "Epoch 924/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0309\n",
            "Epoch 925/1000\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.0327\n",
            "Epoch 926/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0315\n",
            "Epoch 927/1000\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.0306\n",
            "Epoch 928/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0293\n",
            "Epoch 929/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0307\n",
            "Epoch 930/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0323\n",
            "Epoch 931/1000\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 0.0298\n",
            "Epoch 932/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0312\n",
            "Epoch 933/1000\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.0305\n",
            "Epoch 934/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0309\n",
            "Epoch 935/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0291\n",
            "Epoch 936/1000\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.0303\n",
            "Epoch 937/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0296\n",
            "Epoch 938/1000\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.0303\n",
            "Epoch 939/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0264\n",
            "Epoch 940/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0299\n",
            "Epoch 941/1000\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.0307\n",
            "Epoch 942/1000\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.0297\n",
            "Epoch 943/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0298\n",
            "Epoch 944/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0290\n",
            "Epoch 945/1000\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.0289\n",
            "Epoch 946/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0291\n",
            "Epoch 947/1000\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.0285\n",
            "Epoch 948/1000\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.0299\n",
            "Epoch 949/1000\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.0287\n",
            "Epoch 950/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0293\n",
            "Epoch 951/1000\n",
            "1/1 [==============================] - 0s 89ms/step - loss: 0.0285\n",
            "Epoch 952/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0288\n",
            "Epoch 953/1000\n",
            "1/1 [==============================] - 0s 92ms/step - loss: 0.0285\n",
            "Epoch 954/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0291\n",
            "Epoch 955/1000\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.0290\n",
            "Epoch 956/1000\n",
            "1/1 [==============================] - 0s 94ms/step - loss: 0.0283\n",
            "Epoch 957/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0277\n",
            "Epoch 958/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0276\n",
            "Epoch 959/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0297\n",
            "Epoch 960/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0272\n",
            "Epoch 961/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0287\n",
            "Epoch 962/1000\n",
            "1/1 [==============================] - 0s 87ms/step - loss: 0.0273\n",
            "Epoch 963/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0270\n",
            "Epoch 964/1000\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.0279\n",
            "Epoch 965/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0276\n",
            "Epoch 966/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0276\n",
            "Epoch 967/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0278\n",
            "Epoch 968/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0268\n",
            "Epoch 969/1000\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.0282\n",
            "Epoch 970/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0286\n",
            "Epoch 971/1000\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.0268\n",
            "Epoch 972/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0289\n",
            "Epoch 973/1000\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.0272\n",
            "Epoch 974/1000\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.0292\n",
            "Epoch 975/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0264\n",
            "Epoch 976/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0286\n",
            "Epoch 977/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0277\n",
            "Epoch 978/1000\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.0276\n",
            "Epoch 979/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0280\n",
            "Epoch 980/1000\n",
            "1/1 [==============================] - 0s 77ms/step - loss: 0.0259\n",
            "Epoch 981/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0265\n",
            "Epoch 982/1000\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.0267\n",
            "Epoch 983/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0269\n",
            "Epoch 984/1000\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.0279\n",
            "Epoch 985/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0278\n",
            "Epoch 986/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0277\n",
            "Epoch 987/1000\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.0275\n",
            "Epoch 988/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0260\n",
            "Epoch 989/1000\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.0265\n",
            "Epoch 990/1000\n",
            "1/1 [==============================] - 0s 85ms/step - loss: 0.0291\n",
            "Epoch 991/1000\n",
            "1/1 [==============================] - 0s 84ms/step - loss: 0.0264\n",
            "Epoch 992/1000\n",
            "1/1 [==============================] - 0s 83ms/step - loss: 0.0286\n",
            "Epoch 993/1000\n",
            "1/1 [==============================] - 0s 88ms/step - loss: 0.0276\n",
            "Epoch 994/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0279\n",
            "Epoch 995/1000\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.0270\n",
            "Epoch 996/1000\n",
            "1/1 [==============================] - 0s 86ms/step - loss: 0.0261\n",
            "Epoch 997/1000\n",
            "1/1 [==============================] - 0s 82ms/step - loss: 0.0261\n",
            "Epoch 998/1000\n",
            "1/1 [==============================] - 0s 79ms/step - loss: 0.0262\n",
            "Epoch 999/1000\n",
            "1/1 [==============================] - 0s 80ms/step - loss: 0.0265\n",
            "Epoch 1000/1000\n",
            "1/1 [==============================] - 0s 81ms/step - loss: 0.0255\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f74c782f3d0>"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def GeneratorCSS(sentences):\n",
        "  sentence_tokens = [tokens + ['<END>', '<PAD>'] for tokens in [sentences.split(' ')]]\n",
        "  tr_input = [list(map(lambda x: selector_token_dict[x], tokens)) for tokens in sentence_tokens][0]\n",
        "  decoded = decode(\n",
        "      model,\n",
        "      tr_input,\n",
        "      start_token = style_token_dict['<START>'],   \n",
        "      end_token = style_token_dict['<END>'],   \n",
        "      pad_token = style_token_dict['<PAD>']   \n",
        "  )\n",
        "\n",
        "  #aqui imprimimos los resultados\n",
        "  print(\"Selector : {} \".format(sentences))\n",
        "  print(\"Style : {}\".format(' '.join(map(lambda x: style_token_dict_inv[x], decoded[1:-1]))))\n"
      ],
      "metadata": {
        "id": "_L8mQl8JvckH"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GeneratorCSS('aside.context')"
      ],
      "metadata": {
        "id": "BF3PLC2BzGiH",
        "outputId": "9be6fcef-6046-4b0b-957a-39659034ae94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selector : aside.context \n",
            "Style : h3,section { display : flex ; align-items : center ; overflow : hidden ; position : relative ; padding : .5rem } \n"
          ]
        }
      ]
    }
  ]
}